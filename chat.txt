We will start with the robustness for image. Now we support four task, text-xtract, text detection, classification, and recognition respectively. For this demo I used the text-xttact for example, because it includes the text detection and text recognition both. 
It’s included four parameters for image robustness, first is the task, we choose the text-xtract here, and its correlation server URL, next we need an image to assessment by our task model, we also need to specific the output path for attacked images and the results file. After we config all the parameter correctly, we can run it and to see the results. To save time, I have already run the results before the demo starts.
For the results, we can see that for every attack method, there would be assessment different variants based on the original image. For metrics, we used the micro service for calculate metrics for different tasks. And here, for text detection, it includes the IOU, MAP, RECALL, PRECISION, and F1_SCORE, The larger these indicators, the better the robustness of the model. And for text recognition, here we used the CER and WER, CER is the character error rate, WER is the word error rate, so for these two metrics, the smaller the better the robustness for the model. 

As for text classification model, which is classify the direction of the text line image. And it includes four metrics, precision, recall, f1_score and accuracy. They are calculating by the micro service for metrics as well. Here we can see that the robustness of these classification model is pretty good according to the results.

Next, I will introduce the robustness for text. Now we support two tasks, ner and text classification models. 
First, let’s look on the text classification model. Here we need to config these six parameters. 
The dataset path, task and it’s correlation api URL, and the number of samples to assessment the model. The method is the attack method for generate the attacked samples. Here we choose the TextFooler as the attack method. TextFooler introduces a novel approach to generating these adversarial examples in the context of text. The primary idea is to apply synonym substitution to replace words in a given text with synonyms that may lead to misclassification by a target model. 
And also need a output path to save the results file. 
For datasets, here we use the sentiment classification dataset include the label names of positive and negative. 
In order to save time, I've pre-run the attack on a subset of 20 examples. Out of those 20, the model initially predicted 3 of them incorrectly; this leads to an accuracy of 85%. TextAttack ran the adversarial attack process on the remaining 17 examples to try to find a valid adversarial perturbation for each one. Out of those 17, no attacks failed, leading to a success rate of 100%. Another way to articulate this is that the robustness of this model is not sufficient to resist attacks on samples.
There also logged some other helpful statistics for this attack. Among the 17 successful attacks, on average, the attack changed 4% of words to alter the prediction and made 241 queries to find a successful perturbation. Across all 20 inputs, the average number of words was 158.
And it can also organize the results into a json format and save it in the output path.
Let’s see some examples that generated by the textfooler, simply replacing 'tired' with 'disposed' in this sample caused the model to incorrectly identify negative as positive. 

Also for NER model, the same parameter we need to config. 
here I used the dataset of conll2003 to test the robustness of NER model.
Same with the classification model, I’ve pre-run the attack on a subset of 20 examples.
We can see that there are 8 skipped samples, means the accuracy is 60%,



Moving on to Adversarial training function, the attack recipes can be used to create new training sets of adversarial examples. After training for several epochs on the clean training set, the attack generates an adversarial version of each input. This perturbed version of the dataset is substituted for the original and is periodically regenerated according to the model’s current weaknesses. The resulting model can be significantly more robust against the attack used during training.
The training logs would be saved in the folder of outputs, include the train logs and the best model. My laptop's performance is not sufficient to train this model. I think this part should be migrated on the GPU workstation. Other blocker for adversarial training is we must get the original model parameters, otherwise it can’t be trained.

We need to format the datasets as CSV files for text data augmentation.
And for config, we should give the path of dataset and the output file path for augmented dataset. Beside these, we can also config the transformations for per example and the recipe for transformation. Now we can support seven methods for transformation.




So let me introduce some background knowledge and the architecture diagram for text robustness. 
First, we need to understand the adversarial examples in NLP. Adversarial examples in NLP are inputs created to deceive AI models. These examples can be crafted by making subtle changes to the text that are imperceptible or semantically neutral to humans but lead to different interpretations by the model. This phenomenon exposes the vulnerability of NLP models in understanding and processing language in the same way humans do. The most important idea in the text robustness is the Adversarial Examples in NLP. Two different ideas of adversarial examples in NLP. The distinction between visual and semantic similarity in adversarial examples is crucial. Visual similarity focuses on making minute alterations at the character level that might not change the meaning for a human reader but can drastically alter the model's prediction. Semantic similarity, on the other hand, involves rephrasing or paraphrasing the text without changing its original intent. Both methods reveal different aspects of model robustness.
TextAttack supports adversarial attacks based in both definitions of indistinguishability. And serves as a valuable tool in evaluating the robustness of NLP models against these types of adversarial attacks. It allows for testing how well a model can withstand attempts to exploit its vulnerabilities through both visually and semantically similar adversarial examples.
Based on the TextAttack, I have developed a pipeline for robustness assessment on NLP model.
The pipeline includes three functions, Adversarial Attacking is for robustness assessment. Adversarial Attacking: This function is critical for evaluating the current robustness of NLP models. By generating adversarial examples based on both visual and semantic similarity, it exposes potential weaknesses in the model's understanding of language.
Adversarial Training: This function involves training the model using the generated adversarial examples. This process is akin to immunizing the model against such attacks, making it more robust and reliable in real-world applications.
Data Augmentation: This step is designed to enhance the model's exposure to a wider range of linguistic variations. By generating new, diverse text samples, the model can learn from a broader set of data, potentially increasing its accuracy and robustness.
Now the pipeline supports the classification model and NER model both. 

