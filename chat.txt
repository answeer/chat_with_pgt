
Training data

Models with a "-c" in the name are trained on two types of fully commercially-friendly data:

Synthetic data generated with Mixtral-8x7B-Instruct-v0.1. I first created a list of 500+ diverse text classification tasks for 25 professions in conversations with Mistral-large. The data was manually curated. I then used this as seed data to generate several hundred thousand texts for these tasks with Mixtral-8x7B-Instruct-v0.1. The final dataset used is available in the synthetic_zeroshot_mixtral_v0.1 dataset in the subset mixtral_written_text_for_tasks_v4. Data curation was done in multiple iterations and will be improved in future iterations.
Two commercially-friendly NLI datasets: (MNLI, FEVER-NLI). These datasets were added to increase generalization.
Models without a "-c" in the name also included a broader mix of training data with a broader mix of licenses: ANLI, WANLI, LingNLI, and all datasets in this list where used_in_v1.1==True.


Corporate Reports 10-K & 10-Q: 2.5B tokens
Earnings Call Transcripts: 1.3B tokens
Analyst Reports: 1.1B tokens

CC-BY-3.0: 1 dataset (VMware/open-instruct)
MIT License: 8 datasets
CC0 1.0 Universal: 1 dataset
No License (public domain): 6 datasets
Apache License 2.0: 5 datasets (alespalla/chatbot_instruction_prompts, HuggingFaceH4/grok-conversation-harmless, Harelix/Prompt-Injection-Mixed-Techniques-2024, OpenSafetyLab/Salad-Data, jackhhao/jailbreak-classification)
CC-BY-4.0: 1 dataset (natolambert/xstest-v2-copy:1_full_compliance)

Size: 13.6m text tokens in ~209k examples with 649k PII tokens (see summary.json)
4 languages, more to come!
English
French
German
Italian
Synthetic data generated using proprietary algorithms
No privacy violations!
Human-in-the-loop validated high quality dataset


