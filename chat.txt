# ('__label__X19ub3RfYW55', '__label__R2VuZXJhbCBFbnF1aXJ5', '__label__Q29uZmlybWF0aW9uIC0gTWlzc2luZyBDb25maXJtYXRpb24=', '__label__U3RhdGljIERhdGEgLSBTU0kgVXBkYXRl', '__label__U3RhdGljIERhdGEgLSBDcmVhdGlvbiBvciBBbWVuZG1lbnQ=')
# [8.86767447e-01 1.13252640e-01 1.00027310e-05 1.00003463e-05 1.00000989e-05]

from flask import Flask, request, Response, send_file
import fasttext
import base64
import os
from operator import itemgetter
import uuid
import ujson as json
import logging
from typing import List, Dict
import warnings
import time
import math
import regex as re
from timeit import default_timer as timer
from datetime import datetime, timedelta
from collections import defaultdict, OrderedDict
from dataclasses import dataclass, asdict, field
from selectolax.parser import HTMLParser
import itertools


app = Flask(__name__)
MODEL_CACHE = {}

global global_variables_dict
global_variables_dict = {
    "gems_ml_combinations": list(set()),
    "trade_productsubgroup_lookup": {},
    "last_picklist_refresh": datetime(1970, 1, 1),
}

unicode_pattern = re.compile("\w+[\u00ff-\u9fff]")
cleanup_pattern = re.compile("[_\-.\s]{2,}")
single_char_pattern = re.compile("\s+[a-zA-Z]\s+")
single_char2_pattern = re.compile("\^[a-zA-Z]\s+")

english_stopwords = {
    "a",
    "about",
    "above",
    "after",
    "again",
    "against",
    "all",
    "am",
    "an",
    "and",
    "any",
    "are",
    "as",
    "at",
    "be",
    "because",
    "been",
    "before",
    "being",
    "below",
    "between",
    "both",
    "but",
    "by",
    "can",
    "caused",
    "confidential",
    "damage",
    "did",
    "do",
    "does",
    "doing",
    "don",
    "down",
    "during",
    "each",
    "external",
    "few",
    "for",
    "formally",
    "from",
    "further",
    "had",
    "has",
    "have",
    "having",
    "he",
    "her",
    "here",
    "hers",
    "herself",
    "him",
    "himself",
    "his",
    "how",
    "i",
    "if",
    "in",
    "intended",
    "into",
    "is",
    "it",
    "its",
    "itself",
    "just",
    "liability",
    "me",
    "more",
    "most",
    "my",
    "myself",
    "no",
    "nor",
    "not",
    "now",
    "of",
    "off",
    "on",
    "once",
    "only",
    "or",
    "other",
    "our",
    "ours",
    "ourselves",
    "out",
    "over",
    "own",
    "prohibited",
    "recipient",
    "s",
    "same",
    "she",
    "should",
    "so",
    "solely",
    "some",
    "such",
    "t",
    "than",
    "that",
    "the",
    "their",
    "theirs",
    "them",
    "themselves",
    "then",
    "there",
    "these",
    "they",
    "this",
    "those",
    "through",
    "to",
    "too",
    "transmitted" "under",
    "until",
    "up",
    "use",
    "very",
    "virus",
    "viruses",
    "was",
    "we",
    "were",
    "what",
    "when",
    "where",
    "which",
    "while",
    "who",
    "whom",
    "why",
    "will",
    "with",
    "you",
    "your",
    "yours",
    "yourself",
    "yourselves",
}


def main():
    predictions = []
    target_keyword = "cash_management"
    result_items = []
    worflow_info = {}

    result_item = {
        "id": "id1213",
        "body": "please help me close this account",
        "keywords": {},
    }
    for folder in os.listdir("/home/squirro/gems_ankesh/data/tools/models"):
        try:
            print(f"{folder} ==> ")
            if "multi" in folder:
                worflow_info[folder] = "multi_value"
            else:
                worflow_info[folder] = "single_value"
            target_kw = f"pred_{target_keyword}"

            model_file = os.path.join(
                "/home/squirro/gems_ankesh/data/tools/models", folder, "fasttext.bin"
            )
            model = fasttext.load_model(model_file)
            labels, scores = model.predict(result_item["body"], k=1)
            for index, label in enumerate(labels):
                clean_label = label.replace("__label__", "")

                # recover the label from bas64
                clean_label = base64.b64decode(clean_label).decode("utf-8")

                predictions.append({clean_label: scores[index]})
                # print(clean_label, scores[index])

        except Exception as e:
            print(f"Exception {e}")
    result_item["keywords"][target_kw] = predictions
    result_items.append(result_item)
    # print(result_items)
    wf_info = worflow_info

    for k, v in wf_info.items():
        ret = result_items
        pred_data = ret[0].get("keywords", {})
        field_data = {}

        for entry in pred_data[f"pred_cash_management"]:
            for value, conf in entry.items():
                field_data[value] = conf

        # first get rid of the counterbalance value wins!
        if v in ["single_value", "multi_value"]:
            # test if the __not value wins
            # highest_conf = 0.0
            # highest_value = ""

            filtered_field_data = {}
            counter_confidence = 0.0

            # punish all values with the __not_confidence
            for value, confidence in field_data.items():
                if value.startswith("__not_"):
                    counter_confidence = round(confidence, 4)
                    break

            if v == "multi_value":
                value_count = len(field_data)

                if value_count > 1:
                    counter_confidence = round(
                        counter_confidence / (value_count - 1), 4
                    )

            for value, confidence in field_data.items():
                # ignore all counter values
                if value.startswith("__not_"):
                    continue

                corrected_confidence = confidence - counter_confidence
                confidence = round(confidence, 4)
                corrected_confidence = round(corrected_confidence, 4)

                if v == "single_value" or corrected_confidence > 0.10:
                    # avoid log spamming for multi_value models
                    print(
                        "item",
                        f"  - {value}:{corrected_confidence} ({confidence})",
                    )

                filtered_field_data[value] = corrected_confidence

            # use filtered value with the __not_ values removed
            field_data = filtered_field_data

        for value, confidence in field_data.items():
            # Manually lower confidence
            if confidence == 1.0:
                confidence = 0.999

            prediction = {
                "value": value,
                "confidence": round(confidence, 4),
                # "training_docs": wf_info["training_stats"].get(value, 300),
                "mode": v,
            }

            predictions.append(prediction)

        predictions = sorted(predictions, key=itemgetter("confidence"))
        predictions.reverse()

        predicted_values = []
        for prediction in predictions:
            if prediction["confidence"] > 0.1:
                print(
                    "item",
                    f"- accepting {model.field}:'{prediction['value']}' - {prediction['confidence']}",
                )

                print.append(
                    format_prediction_custom(
                        model,
                        prediction["value"],
                        model.field,
                        prediction["confidence"],
                        prediction["training_docs"],
                    )
                )

                predicted_values.append(prediction["value"])

        predicted_values = list(set(predicted_values))

        # # First store into a temp dict
        # model_dict = {}
        # for ml_result in ml_results:
        # fieldname = ml_result["field"]
        # model_dict.setdefault(fieldname, [])
        # model_dict[fieldname].append(ml_result)

        # for k, v in model_dict.items():
        # model_index = find_index_ml_predictions(k, item["ml_predictions"])
        # if not model_index:
        #     item["ml_predictions"].append(v)
        # else:
        #     # If prediction found, append to correct index
        #     # Manually reduce to -1. Possibly split up functions
        #     model_index -= 1
        #     # log.warning(f"Model Index pos is {model_index} for {k}")
        #     item["ml_predictions"][model_index].extend(v)

        # return item


def format_prediction_custom(
    model, value=None, field=None, confidence=None, training_docs=None
):
    """
    Format Prediction
    :returns dictionary
    """

    return {
        "value": value,
        "field": field,
        "confidence": confidence,
        "training_docs": training_docs,
        "model": model.name,
        "model_version": model.published_version,
    }


@app.route("/ml_migrate/predict/<profile>", methods=["POST"])
def predict_api(profile):
    # generate a transaction uuid for tracking purposes
    transaction_id = str(uuid.uuid1())
    fqdn = "1142-Ankesh"

    # response envelope
    ret = {
        "items": [],
        "msg": "OK",
        "transaction_id": transaction_id,
        "messages": [f"Hostname: {fqdn}"],
    }

    # Default f_item to be used when prediction fails
    # default_f_item = [
    #     {"id": "Unknown", "created_at": "", "predictions": [], "inscope_fields": []}
    # ]
    default_f_item = [{"id": "Unknown", "created_at": "", "predictions": []}]

    try:
        start = timer()
        created_at_str = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
        debug = True
        final_keywords = {}
        req_item_id = "not_set"
        email_id = "Unknown"

        pretty = True
        if pretty:
            indent = 2
        else:
            indent = 0

        strategy = "default"

        # always fall back to the default strategy
        if not strategy:
            strategy = "default"

        strategy_version = 83

        if strategy_version:
            strategy_version = int(strategy_version)

        # with open(
        #     "request.json", "r"
        # ) as file:
        #     request = json.load(file)

        # get updated req (possibly due to bad json formatting) and ret (with new messages)
        req, ret = process_prediction_req_data_api(transaction_id, request, ret)

        items = req.get("items")

        if isinstance(items[0], list):
            item = items[0][0]
        else:
            item = items[0]

        if item.get("id"):
            default_f_item[0]["id"] = item.get("id")

        # decrypt the token
        # ZQ Fix to fix brackets sent in profile value
        profile = profile.replace("{", "").replace("}", "").lower().strip()
        api_token = "replace_with_custom_token"

        if not api_token:
            raise Exception("token not valid or user not found")
        else:
            ret["messages"].append(f"Using {fqdn}")

            if "standardchartered.com" in fqdn:
                cluster_uri = f"https://{fqdn}:8453"
            else:
                cluster_uri = f"http://{fqdn}:81"

            ret["messages"].append(f"Squirro Client URI {cluster_uri}")

        if strategy_version:
            # specific strategy version requests
            ml_strategy_version = load_ml_strategy_version(
                profile, strategy, strategy_version
            )
        else:
            # using the published version
            ml_strategy = load_ml_strategy(profile, strategy)

            if ml_strategy.published_version:
                ml_strategy_version = load_ml_strategy_version(
                    profile, strategy, ml_strategy.published_version
                )
            else:
                # fallback to the latest version
                ml_strategy_version = load_ml_strategy_version(
                    profile, strategy, ml_strategy.max_version
                )

        if not ml_strategy_version:
            raise Exception(f"No strategy definition found for profile {profile}")

        print(
            f'Predicting item {item["id"]} with strategy {ml_strategy_version.name} v{ml_strategy_version.version} transaction: {transaction_id}'
        )
        req_item_id = item["id"]
        keywords = defaultdict(set)
        norm_mode = "alphanum"
        ret["messages"].append(f"norm_mode: {norm_mode}")

        item_keywords = item.get("keywords", {})

        if isinstance(item_keywords, list):
            print("Fixing the keywords format from list do dict")
            item_keywords = item_keywords[0]

        item["body"] = prepare_item_data(
            req_item_id,
            item.get("title", ""),
            item.get("body", ""),
            item_keywords,
            norm_mode=norm_mode,
        )

        # fix bad keywords format
        if isinstance(item.get("keywords"), list):
            item["keywords"] = item["keywords"][0]

        # fix bad keyword names
        if item.get("keywords"):
            if item["keywords"].get("originating_country"):
                item["keywords"]["sroriginatingcountry"] = item["keywords"].get(
                    "originating_country"
                )

            if item["keywords"].get("receiver_mailbox"):
                item["keywords"]["email_id"] = item["keywords"].get("receiver_mailbox")

            if item["keywords"].get("email_id"):
                item["body"] = (
                    item["keywords"]["email_id"][0].replace("@", "_").replace(".", "_")
                    + " "
                    + item["body"]
                )

        # collect all keywords for analytics
        kws = item.get("keywords", {})
        for key, value in kws.items():
            if key in ["sroriginatingcountry", "email_id"]:
                continue
            keywords[key].update(value)

        for key, value in keywords.items():
            final_keywords[key] = list(value)

        email_id = kws.get("receiver_mailbox", ["Unknown"])[0]

        predictions = {}

        # initialize all output fields as empty dicts
        # load strategy

        for step in ml_strategy_version.strategy:
            if step["step"] == "predict":
                predictions[step["output_field"]] = {}

        item = process_strategy(
            profile,
            ml_strategy_version,
            item,
            global_variables_dict,
        )

        # for now just add the first entry of each
        for model_results in item["ml_predictions"]:
            predictions.update(format_prediction(model_results))

        f_item = {
            "id": item["id"],
            "predictions": predictions,
            # "inscope_fields": item["inscope_fields"],
        }

        ret["items"] = [f_item]

    except Exception as e:
        ret["msg"] = str(e)
        ret["items"] = default_f_item
        print(e)
    finally:
        end = timer()
        ret["took_ms"] = int((end - start) * 1000)
        # check if ret[items] is populated, if not, add a error fallback

        log_payload = {
            "a": "req",
            "p": {
                "transaction_id": transaction_id,
                "response": ret,
                "request": {
                    "profile": profile,
                    "ip": "TBA - 1142",
                    "user_agent": "TBA - request.user_agent.string",
                    "request_length": "TBA - calc length",
                    "keywords": final_keywords,
                    "item_id": req_item_id,
                    "request_rate": 2,
                    "email_id": email_id,
                },
                "created_at": created_at_str,
            },
        }

        # handling cases if something goes really bad
        try:
            log_payload["p"]["debug"] = item.get("ml_messages")
            log_payload["p"]["request"]["strategy"] = ml_strategy_version.name
            log_payload["p"]["request"][
                "strategy_version"
            ] = f"{ml_strategy_version.name} v{ml_strategy_version.version}"
        except Exception:  # nosec
            print("Critical error happened: %r", log_payload)
        print(json.dumps(ret, indent=indent))

        return Response(
            json.dumps(ret, indent=indent),
            status=200,
            mimetype="application/json",
        )


def predict(profile):
    # generate a transaction uuid for tracking purposes
    transaction_id = str(uuid.uuid1())
    fqdn = "1142-Ankesh"

    # response envelope
    ret = {
        "items": [],
        "msg": "OK",
        "transaction_id": transaction_id,
        "messages": [f"Hostname: {fqdn}"],
    }

    # Default f_item to be used when prediction fails
    # default_f_item = [
    #     {"id": "Unknown", "created_at": "", "predictions": [], "inscope_fields": []}
    # ]
    default_f_item = [{"id": "Unknown", "created_at": "", "predictions": []}]

    # try:
    if True:
        start = timer()
        created_at_str = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
        debug = True
        final_keywords = {}
        req_item_id = "not_set"
        email_id = "Unknown"

        pretty = True
        if pretty:
            indent = 2
        else:
            indent = 0

        strategy = "default"

        # always fall back to the default strategy
        if not strategy:
            strategy = "default"

        strategy_version = 83

        if strategy_version:
            strategy_version = int(strategy_version)

        with open(
            "request.json", "r"
        ) as file:
            request = json.load(file)

        # get updated req (possibly due to bad json formatting) and ret (with new messages)
        req, ret = process_prediction_req_data(transaction_id, request, ret)

        items = req.get("items")

        if isinstance(items[0], list):
            item = items[0][0]
        else:
            item = items[0]

        if item.get("id"):
            default_f_item[0]["id"] = item.get("id")

        # decrypt the token
        # ZQ Fix to fix brackets sent in profile value
        profile = profile.replace("{", "").replace("}", "").lower().strip()
        api_token = "replace_with_custom_token"

        if not api_token:
            raise Exception("token not valid or user not found")
        else:
            ret["messages"].append(f"Using {fqdn}")

            if "standardchartered.com" in fqdn:
                cluster_uri = f"https://{fqdn}:8453"
            else:
                cluster_uri = f"http://{fqdn}:81"

            ret["messages"].append(f"Squirro Client URI {cluster_uri}")

        if strategy_version:
            # specific strategy version requests
            ml_strategy_version = load_ml_strategy_version(
                profile, strategy, strategy_version
            )
        else:
            # using the published version
            ml_strategy = load_ml_strategy(profile, strategy)

            if ml_strategy.published_version:
                ml_strategy_version = load_ml_strategy_version(
                    profile, strategy, ml_strategy.published_version
                )
            else:
                # fallback to the latest version
                ml_strategy_version = load_ml_strategy_version(
                    profile, strategy, ml_strategy.max_version
                )

        if not ml_strategy_version:
            raise Exception(f"No strategy definition found for profile {profile}")

        print(
            f'Predicting item {item["id"]} with strategy {ml_strategy_version.name} v{ml_strategy_version.version} transaction: {transaction_id}'
        )
        req_item_id = item["id"]
        keywords = defaultdict(set)
        norm_mode = "alphanum"
        ret["messages"].append(f"norm_mode: {norm_mode}")

        item_keywords = item.get("keywords", {})

        if isinstance(item_keywords, list):
            print("Fixing the keywords format from list do dict")
            item_keywords = item_keywords[0]

        item["body"] = prepare_item_data(
            req_item_id,
            item.get("title", ""),
            item.get("body", ""),
            item_keywords,
            norm_mode=norm_mode,
        )

        # fix bad keywords format
        if isinstance(item.get("keywords"), list):
            item["keywords"] = item["keywords"][0]

        # fix bad keyword names
        if item.get("keywords"):
            if item["keywords"].get("originating_country"):
                item["keywords"]["sroriginatingcountry"] = item["keywords"].get(
                    "originating_country"
                )

            if item["keywords"].get("receiver_mailbox"):
                item["keywords"]["email_id"] = item["keywords"].get("receiver_mailbox")

            if item["keywords"].get("email_id"):
                item["body"] = (
                    item["keywords"]["email_id"][0].replace("@", "_").replace(".", "_")
                    + " "
                    + item["body"]
                )

        # collect all keywords for analytics
        kws = item.get("keywords", {})
        for key, value in kws.items():
            if key in ["sroriginatingcountry", "email_id"]:
                continue
            keywords[key].update(value)

        for key, value in keywords.items():
            final_keywords[key] = list(value)

        email_id = kws.get("receiver_mailbox", ["Unknown"])[0]

        predictions = {}

        # initialize all output fields as empty dicts
        # load strategy

        for step in ml_strategy_version.strategy:
            if step["step"] == "predict":
                predictions[step["output_field"]] = {}

        item = process_strategy(
            profile,
            ml_strategy_version,
            item,
            global_variables_dict,
        )

        # for now just add the first entry of each
        for model_results in item["ml_predictions"]:
            predictions.update(format_prediction(model_results))

        f_item = {
            "id": item["id"],
            "predictions": predictions,
            # "inscope_fields": item["inscope_fields"],
        }

        ret["items"] = [f_item]

        # except Exception as e:
        #     ret["msg"] = str(e)
        #     ret["items"] = default_f_item
        #     print(e)
        # finally:
        #     end = timer()
        #     ret["took_ms"] = int((end - start) * 1000)
        #     # check if ret[items] is populated, if not, add a error fallback

        log_payload = {
            "a": "req",
            "p": {
                "transaction_id": transaction_id,
                "response": ret,
                "request": {
                    "profile": profile,
                    "ip": "TBA - 1142",
                    "user_agent": "TBA - request.user_agent.string",
                    "request_length": "TBA - calc length",
                    "keywords": final_keywords,
                    "item_id": req_item_id,
                    "request_rate": 2,
                    "email_id": email_id,
                },
                "created_at": created_at_str,
            },
        }

        # handling cases if something goes really bad
        try:
            log_payload["p"]["debug"] = item.get("ml_messages")
            log_payload["p"]["request"]["strategy"] = ml_strategy_version.name
            log_payload["p"]["request"][
                "strategy_version"
            ] = f"{ml_strategy_version.name} v{ml_strategy_version.version}"
        except Exception:  # nosec
            print("Critical error happened: %r", log_payload)
        print(json.dumps(ret, indent=indent))

        # return Response(
        #     json.dumps(ret, indent=indent),
        #     status=200,
        #     mimetype="application/json",
        # )


# ----------------------------------------
# Strategy
# ----------------------------------------
def process_strategy(
    profile,
    strategy,
    item,
    global_variables_dict,
):
    """
    Process individual strategies and items
    :returns response
    """
    step_count = 0

    item.setdefault("ml_predictions", [])
    item.setdefault("ml_messages", [])
    item.setdefault("processed_strategies", [])

    # To review later
    combination_model_list = [
        "productgroup",
        "productsubgroup",
        "productname",
        "querytype",
    ]

    # To review later, combo needs to be part of strategy to differentiate
    non_combination_model_list = ["classification"]

    # inscope_fields = set()

    for input_field in strategy.input_fields:
        if (
            input_field in combination_model_list
            or input_field in non_combination_model_list
        ):
            if item["keywords"].get(input_field):
                row = {
                    "value": item["keywords"].get(input_field)[0],
                    "field": input_field,
                    "confidence": 1.00,
                    "training_docs": -1,
                    "model": "input_value",
                    "model_version": 1,
                }
                item["ml_predictions"].append([row])

    for step in strategy.strategy:
        step_count += 1
        step_type = step.get("step")

        # Check if result is already known
        if skip_step(item, step):
            if step_type == "predict":
                skip_message = f"Skipping predict step for model {step.get('model')}"
            elif step_type == "process_strategy":
                skip_message = f"Skipping proecess_strategy step for strategy {step.get('strategy')}"
            elif step_type == "custom":
                skip_message = (
                    f"Skipping custom step for function {step.get('function')}"
                )

            else:
                skip_message = (
                    f"Skipping unsupported step of type {step_type}: {repr(step)}"
                )

            log_strategy(
                item,
                message=f"{strategy.name}_{strategy.version} step {step_count}: {skip_message}",
            )
            continue

        if step["step"] == "process_strategy":
            sub_strategy_name = step.get("strategy")
            log_strategy(
                item,
                message=f"{strategy.name}_{strategy.version} step {step_count}: process sub-strategy {sub_strategy_name}",
            )

            # recursion prevention
            if sub_strategy_name in item["processed_strategies"]:
                log_strategy(
                    item,
                    message=f"- Possible recursion of process_strategy detected, strategy {sub_strategy_name} as already been run. Aborting",
                    level="warn",
                )
                continue

            # prevent recursions
            item["processed_strategies"].append(sub_strategy_name)

            sub_strategy = load_ml_strategy(profile, sub_strategy_name)
            sub_strategy_version = load_ml_strategy_version(
                profile, sub_strategy_name, version=sub_strategy.published_version
            )
            if not sub_strategy:
                log_strategy(
                    item,
                    message=f"- Failed to load strategy {sub_strategy_name}, skipping step",
                )
                continue

            log_strategy(
                item,
                message=f"- Running sub-strategy {sub_strategy.name} version {sub_strategy_version.version}",
            )

            # run the sub strategy
            item = process_strategy(
                profile,
                sub_strategy_version,
                item,
                global_variables_dict,
            )

        elif step["step"] == "predict":
            log_strategy(
                item,
                message=f"{strategy.name}_{strategy.version} step {step_count}: predict model {step.get('model')}",
            )
            model_name = step.get("model")

            # add the inscope values
            # inscope_fields.add(step["output_field"])
            item = run_machinelearning_workflow(
                profile,
                model_name,
                item,
                mode="fasttext_direct",
            )

            # If model not in combination set
            if step["output_field"] in non_combination_model_list:
                model_index = find_index_ml_predictions(
                    step["output_field"], item["ml_predictions"]
                )
                # manually lower index see find_index_ml_prediction. TODO splitup functions
                model_index = model_index - 1
                # log.info(
                #    f"Found model index: {model_index} for non combo field {step['output_field']}"
                # )
                # log.warning(f'Trying with {item["ml_predictions"][model_index]}')

                # Find prediction with highest confidence
                most_confident_row = max(
                    item["ml_predictions"][model_index], key=lambda x: x["confidence"]
                )

                # append current values
                item["ml_predictions"][model_index].append(most_confident_row)

        elif step["step"] == "custom":
            function_name = step.get("function")
            log_strategy(
                item,
                message=f"{strategy.name}_{strategy.version} step {step_count}: running {function_name}",
            )
            try:
                # investigate why below is not working
                # custom_function = getattr(custom_code, function_name)
                # Check for kwargs
                if step.get("kwargs"):
                    item = eval(  # nosec
                        function_name + "(item, global_variables_dict, step['kwargs'])"
                    )  # nosec
                else:
                    item = eval(  # nosec
                        function_name + "(item, global_variables_dict)"
                    )  # nosec

                # add the inscope fields
                # for prediction in item["ml_predictions"]:
                #     for pred in prediction:
                #         inscope_fields.add(pred["field"])

            except Exception as e:
                # improved error handling
                print(e)
                log_strategy(item, message=e, level="exception")
                log_strategy(
                    item, message=f"- unable to execute {function_name}", level="warn"
                )
                pass

    # Convert inscope_fields set back to []
    # item["inscope_fields"] = list(inscope_fields)

    return item


def run_machinelearning_workflow(profile, name, item, mode="fasttext_direct"):
    """
    Helper function to run a model inference and to return the results
    """
    print(f"loaded model {name}")
    model = load_ml_model(profile=profile, name=name)

    training_docs_list = []

    if item["keywords"].get(model.field):
        log_strategy(
            item,
            message=f"Skipping model {model.name}, target field is already known in the item",
        )

        return item

    workflow_info = {}
    workflow_ids = []
    for wf in model.published_workflow_ids:
        workflow_ids.append(wf["workflow_id"])
        workflow_info[wf["workflow_id"]] = wf

        for doc_count in wf["training_stats"].values():
            training_docs_list.append(doc_count)

    ml_results = []
    predictions = []

    # ensuring legacy models to work

    print("predict mode: %s", mode)

    for workflow_id in workflow_ids:
        try:
            wf_info = workflow_info.get(workflow_id)

            if wf_info.get("workflow_name"):
                log_strategy(
                    item,
                    message=f"- running {wf_info['mode']} workflow {wf_info['workflow_name']}",
                )
            else:
                log_strategy(
                    item,
                    message=f"- running workflow id {workflow_id}",
                )

            ret = predict_fasttext(workflow_id, model.field, {"items": [item]})

        except Exception as e:
            print(
                "Failed to run workflow id %s for field %s",
                model.published_workflow_id,
                model.name,
            )
            print(e)
            raise Exception(
                "Failed to run workflow id {0} for field {1}".format(
                    model.published_workflow_id, model.name
                )
            )

        ret = ret.get("items", [])

        if not ret:
            raise Exception(
                f"ml workflow {model.name} returnd no prediction for item {item['id']}"
            )

        pred_data = ret[0].get("keywords", {})
        field_data = {}

        for entry in pred_data[f"pred_{model.field}"]:
            for value, conf in entry.items():
                field_data[value] = conf

        # first get rid of the counterbalance value wins!
        if wf_info["mode"] in ["single_value", "multi_value"]:
            # test if the __not value wins
            # highest_conf = 0.0
            # highest_value = ""

            filtered_field_data = {}
            counter_confidence = 0.0

            # punish all values with the __not_confidence
            for value, confidence in field_data.items():
                if value.startswith("__not_"):
                    counter_confidence = round(confidence, 4)
                    break

            if wf_info["mode"] == "multi_value":
                value_count = len(field_data)

                if value_count > 1:
                    counter_confidence = round(
                        counter_confidence / (value_count - 1), 4
                    )

            for value, confidence in field_data.items():
                # ignore all counter values
                if value.startswith("__not_"):
                    continue

                corrected_confidence = confidence - counter_confidence
                confidence = round(confidence, 4)
                corrected_confidence = round(corrected_confidence, 4)

                if wf_info["mode"] == "single_value" or corrected_confidence > 0.10:
                    # avoid log spamming for multi_value models
                    log_strategy(
                        item,
                        message=f"  - {value}:{corrected_confidence} ({confidence})",
                    )

                filtered_field_data[value] = corrected_confidence

            # use filtered value with the __not_ values removed
            field_data = filtered_field_data

        for value, confidence in field_data.items():
            # Manually lower confidence
            if confidence == 1.0:
                confidence = 0.999

            prediction = {
                "value": value,
                "confidence": round(confidence, 4),
                "training_docs": wf_info["training_stats"].get(value, 300),
                "mode": wf_info["mode"],
            }

            predictions.append(prediction)

    predictions = sorted(predictions, key=itemgetter("confidence"))
    predictions.reverse()

    predicted_values = []
    for prediction in predictions:
        if prediction["confidence"] > 0.1:
            log_strategy(
                item,
                message=f"- accepting {model.field}:'{prediction['value']}' - {prediction['confidence']}",
            )

            ml_results.append(
                format_prediction_custom(
                    model,
                    prediction["value"],
                    model.field,
                    prediction["confidence"],
                    prediction["training_docs"],
                )
            )

            predicted_values.append(prediction["value"])

    predicted_values = list(set(predicted_values))

    # First store into a temp dict
    model_dict = {}
    for ml_result in ml_results:
        fieldname = ml_result["field"]
        model_dict.setdefault(fieldname, [])
        model_dict[fieldname].append(ml_result)

    for k, v in model_dict.items():
        model_index = find_index_ml_predictions(k, item["ml_predictions"])
        if not model_index:
            item["ml_predictions"].append(v)
        else:
            # If prediction found, append to correct index
            # Manually reduce to -1. Possibly split up functions
            model_index -= 1
            # log.warning(f"Model Index pos is {model_index} for {k}")
            item["ml_predictions"][model_index].extend(v)

    return item


def process_prediction_req_data(transaction_id, request, ret):
    """
    Helper function to process response
    :return ret data
    """
    req = None
    try:
        req = request["data"]
    except Exception:
        ret["messages"].append("Recieved invalid json")

    if not req:
        req_str = request.data.decode("utf-8").strip()
        try:
            start_str = '"body":"'
            end_str = '"}],"token"'

            body_start = req_str.find(start_str)

            if body_start > -1:
                body_start += len(start_str)

            body_end = req_str.find(end_str)

            cleaned_item = req_str[0:body_start]
            cleaned_item += req_str[body_end:]

            # trying to parse the item wihout the body
            req = json.loads(cleaned_item)

            # getting the non-json body into the object
            body = req_str[body_start:body_end]
            req["items"][0]["body"] = body
            ret["messages"].append("invalid json fixup worked")
        except Exception:
            print(
                "Fixup attempt for transaction %s failed",
                transaction_id,
                exc_info=True,
            )
            print("Trying to recover as much as possible from req")
            try:
                req = find_missing_req_values(req_str)
                ret["messages"].append("Invalid json with some metadata found")
                ret["msg"] = "error"
            except Exception:
                raise Exception(
                    "Failed to parse json, fixup workaround failed and unable to find any item metadata"
                )

    if not req:
        raise Exception("post data is not json?")

    if not req.get("token"):
        raise Exception("token is missing")

    if not req.get("items"):
        raise Exception("items is missing in request json payload or empty")

    if len(req.get("items")) > 1:
        raise Exception("Received more than 1 item, this is currently not supported")

    return req, ret


def process_prediction_req_data_api(transaction_id, request, ret):
    """
    Helper function to process response
    :return ret data
    """
    req = None
    try:
        req = json.loads(request.data)
    except Exception:
        ret["messages"].append("Recieved invalid json")

    if not req:
        req_str = request.data.decode("utf-8").strip()
        try:
            start_str = '"body":"'
            end_str = '"}],"token"'

            body_start = req_str.find(start_str)

            if body_start > -1:
                body_start += len(start_str)

            body_end = req_str.find(end_str)

            cleaned_item = req_str[0:body_start]
            cleaned_item += req_str[body_end:]

            # trying to parse the item wihout the body
            req = json.loads(cleaned_item)

            # getting the non-json body into the object
            body = req_str[body_start:body_end]
            req["items"][0]["body"] = body
            ret["messages"].append("invalid json fixup worked")
        except Exception:
            print(
                "Fixup attempt for transaction %s failed",
                transaction_id,
                exc_info=True,
            )
            print("Trying to recover as much as possible from req")
            try:
                req = find_missing_req_values(req_str)
                ret["messages"].append("Invalid json with some metadata found")
                ret["msg"] = "error"
            except Exception:
                raise Exception(
                    "Failed to parse json, fixup workaround failed and unable to find any item metadata"
                )

    if not req:
        raise Exception("post data is not json?")

    if not req.get("token"):
        raise Exception("token is missing")

    if not req.get("items"):
        raise Exception("items is missing in request json payload or empty")

    if len(req.get("items")) > 1:
        raise Exception("Received more than 1 item, this is currently not supported")

    return req, ret


def find_missing_req_values(req_str):
    """
    Helper function to extract elements from bad data
    :return request, ret
    """
    default_req = {
        "token": "",
        "items": [{"id": "", "created_at": "", "keywords": {}}],
    }

    req_str = req_str.replace(" ", "")
    req_str = req_str.replace("\\", "")

    print(f"req str is {req_str}")

    id_pattern = r"\"?id\"?:\s?\"([A-Z]*-[A-Z]*-[0-9]*-[A-Z]*[0-9]*)\""

    if "id" in req_str:
        match = re.search(id_pattern, req_str)
        if match:
            default_req["items"][0]["id"] = match.group(1)

    # log.warning(default_req)
    return default_req


def prepare_item_data(
    item_id, title, data, keywords, prepend_keywords=[], norm_mode="alphanum"
):
    """
    Shared function used by both training and inference to prepare the content for the model
    """

    # log.info(f"prepare_item_data: {norm_mode}")

    length = len(data)

    if length > 102400:
        print(f"{item_id}: truncating large data of {length} bytes to 100kb")
        data = data[0:102400]

    clean_parts = []
    keyword_values = []

    for kw in prepend_keywords:
        kw_value = keywords.get(kw)

        if kw_value and len(kw_value) > 0:
            keyword_values.append(kw_value[0])

    # fix badly named keywords from the old days
    if keyword_values:
        clean_parts.append(clean_text(" ".join(keyword_values), mode="email"))

    # clean the title
    if title:
        clean_parts.append(clean_text(title, mode=norm_mode))

    if data:
        clean_parts.append(clean_html(data, mode=norm_mode))

    clean_data = " ".join(clean_parts)
    # normalised_length = len(clean_data)

    # log.info(f"{item_id}: clean result from {length} -> {normalised_length}")

    return clean_data.strip()


def clean_html(html, strip_html_only=False, skip_html_strip=False, mode="alpha"):
    # log.info("clean html mode: %s", mode)

    if skip_html_strip:
        body = html
    else:
        unwanted_tags = [
            "head",
            "style",
            "script",
            "xmp",
            "iframe",
            "noembed",
            "noframes",
        ]

        tree = HTMLParser(html)
        tree.strip_tags(unwanted_tags)
        node = tree.root
        body = node.text()

    if strip_html_only:
        return body

    body = clean_text(body, mode=mode)
    return body


def clean_text(text, mode="alpha"):
    if mode == "alpha":
        alphanum_pattern = re.compile("[^A-Za-z0-9\s]+")
        whitespace_pattern = re.compile("\s+")

        cleansed_text = " ".join(
            word
            for word in text.split()
            if "@" not in word and "http" not in word and "www" not in word
        )
        cleansed_text = alphanum_pattern.sub("", cleansed_text)
        cleansed_text = whitespace_pattern.sub(" ", cleansed_text)
        cleansed_text = " ".join(
            word for word in cleansed_text.split() if word.isalpha()
        )
        cleansed_text = cleansed_text.lower()

        return cleansed_text

    elif mode == "alphanum":
        document = unicode_pattern.sub(" ", text)
        document = re.sub(r"\W", " ", document)

        # remove all single characters
        document = single_char_pattern.sub(" ", document)

        # Remove single characters from the start
        document = single_char2_pattern.sub(" ", document)

        # Substituting multiple spaces with single space
        document = re.sub(r"\s+", " ", document, flags=re.I)

        # replace multiple spaces and special character
        document = cleanup_pattern.sub(" ", document)

        # Converting to Lowercase
        document = document.lower()

        cleansed_text = []

        for word in document.split():
            if len(word) > 1 and word not in english_stopwords:
                cleansed_text.append(word)

        return " ".join(cleansed_text)
    elif mode == "unicode":
        text = text.lower()

        # cleaned_str = ''.join(c if (c.isalnum() or c == '_') and not ('\u00ff' <= c <= '\u9fff') else '_' if c in [".", ",", "-", ":", ";", "@", "/"] else ' ' for c in text)
        cleaned_str = "".join(
            (
                c
                if (c.isalnum() or c == "_")
                else "_" if c in [".", ",", "-", ":", ";", "@", "/"] else " "
            )
            for c in text
        )

        filtered_words = []

        for word in cleaned_str.split():
            if len(word) == 1:
                continue

            consecutive_special_count = 0
            skip_word = False
            for char in word:
                if char == "_":
                    consecutive_special_count += 1
                    if consecutive_special_count >= 2:
                        skip_word = True
                        break
                else:
                    consecutive_special_count = 0

            if skip_word:
                continue

            word = word.strip("_")

            filtered_words.append(word)
        cleansed_text = " ".join(filtered_words)

        filtered_words = []
        for word in cleansed_text.split():
            if word in english_stopwords:
                continue

            filtered_words += word.split("_")
        cleansed_text = " ".join(filtered_words)

        return cleansed_text
    elif mode == "email":
        return text.replace("@", "_").replace(".", "_").strip()
    else:
        return f"clean_text mode {mode} is not supported."


def format_prediction(model_results):
    to_return = {}
    if isinstance(model_results, list):
        for pred in model_results:
            pred_value_name = pred["field"]
            to_return[pred_value_name] = {
                "value": pred["value"],
                "confidence": pred["confidence"],
            }

    elif isinstance(model_results, dict):
        pred_value_name = model_results["field"]
        to_return[pred_value_name] = {
            "value": model_results["value"],
            "confidence": model_results["confidence"],
        }

    return to_return


def skip_step(item, step):
    """
    Check if strategy should be skipped based on item prediction metadata
    :return bool
    """
    skip_step = False
    # Condition handling for conditions
    if step.get("conditions"):
        print("Found item predictions and step condition. Processing predictions now")

        # By default ensure conditions are treated as AND
        if step.get("conditions_type") == "AND" or not step.get("conditions_type"):
            # Loop through conditions
            # Assume that all conditions must be true for step to be skipped
            skip_step = all(
                [
                    process_condition(condition, item)
                    for condition in step.get("conditions")
                ]
            )

        elif step.get("conditions_type") == "OR":
            skip_step = any(
                [
                    process_condition(condition, item)
                    for condition in step.get("conditions")
                ]
            )

        if not skip_step:
            return True
        else:
            return False


def process_condition(condition, item):
    """
    Process conditions within steps based on condition type and ml prediction
    :returns bool
    """

    # log.info("condition: %r", condition)

    condition_flag = True
    field = condition["field"]
    ml_predictions = item["ml_predictions"]

    # Here we only look for the key and dont care about values
    if condition["type"] == "is_defined":
        # log_strategy(item, message="Processing condition 'is_defined'")

        # if no predictions yet, it can only be false
        if not ml_predictions:
            return False

        if find_index_ml_predictions(field, ml_predictions):
            # log_strategy(
            #    item,
            #    message=f"condition passed. found condition model {field} in ml predictions",
            # )
            condition_flag = True
        else:
            # log_strategy(
            #    item,
            #    message=f"condition failed. unable to find condition model {field} in ml predictions",
            # )
            condition_flag = False

    # Here we dont care about values as well since model is not present
    elif condition["type"] == "is_undefined":
        # log_strategy(item, message="Prossing condition 'is_undefined'")

        # if no predictions, then it can only be True
        if not ml_predictions:
            return True

        if not find_index_ml_predictions(field, ml_predictions):
            log_strategy(
                item,
                message=f"condition passed. unable to find model {field} in ml predictions",
            )
            condition_flag = True
            # break
        else:
            log_strategy(
                item,
                message=f"condition failed. found model {field} in ml prediction",
            )
            condition_flag = False

    # Here we care about exact macth with the hightest confidence
    elif condition["type"] == "is_equals_top_value":
        # log_strategy(item, message="processing condition 'is_equals_top_value'")

        # if no predictions are mode it can only be false
        if not ml_predictions:
            return False

        condition_value = condition.get("value")
        ml_condition_flags = []

        # First make sure we have the right model and value
        # Need to change
        for model in ml_predictions:
            # Find list of models
            if field == model[0]["field"]:
                for prediction in model:
                    # for model, predictions in ml_predictions.items():
                    # for ml_pred_k, ml_pred_v in row.items():
                    if (
                        field == prediction["field"]
                        and condition_value == prediction["value"]
                    ):
                        if check_top_confidence_value(
                            field, condition_value, ml_predictions
                        ):
                            # log_strategy(
                            #    item,
                            #    message=f"condition passed. found exact match with highest confidence"
                            #    f"for {condition_value}",
                            # )
                            ml_condition_flags.append(True)
                        else:
                            # log_strategy(
                            #    item,
                            #    message=f"condition failed. exact match found, however {condition_value} "
                            #    f"does not have the highest confidence value",
                            # )
                            ml_condition_flags.append(False)
                    else:
                        # log_strategy(
                        #    item,
                        #    message=f"condition failed. unable to find match for condition {field}:{condition_value} "
                        #    f"in prediction {prediction['field']}:{prediction['value']}",
                        # )
                        ml_condition_flags.append(False)

        # log.info("ml_condition_flags: %r", ml_condition_flags)

        # Check for any negative values
        # Improve with while loop
        if not all(ml_condition_flags):
            condition_flag = False

    elif condition["type"] == "is_no_confidence_above":
        # log_strategy(item, message="processing condition 'is_no_confidence_above'")

        ml_condition_flags = []
        condition_confidence = condition.get("value")
        # First make sure we have the right model and value
        # Need to change
        for model in ml_predictions:
            for prediction in model:
                # for model, predictions in ml_predictions.items():
                # for ml_pred_k, ml_pred_v in row.items():
                if (
                    field == prediction["field"]
                    and condition_confidence >= prediction["confidence"]
                ):
                    # log_strategy(
                    #    item,
                    #    message=f"condition passed. predicted confidence {prediction['confidence']} "
                    #    f"does not exceed {condition_confidence}",
                    # )
                    ml_condition_flags.append(True)
                else:
                    # log_strategy(
                    #    item,
                    #    message=f"condition failed. predicted confidence {prediction['confidence']} "
                    #    f"exceeds {condition_confidence}",
                    # )
                    ml_condition_flags.append(False)
                    condition_flag = False

    elif condition["type"] == "is_confidence_above":
        log_strategy(item, message="Processing conditions 'is_confidence_above'")

        ml_condition_flags = []
        conditon_confidence = condition.get("value")
        # First make sure we have the right model and value
        # Need to change
        for model in ml_predictions:
            for prediction in model:
                # for model, predictions in ml_predictions.items():
                # for ml_pred_k, ml_pred_v in row.items():
                if (
                    field == prediction["field"]
                    and conditon_confidence < prediction["confidence"]
                ):
                    # log_strategy(
                    #    item,
                    #    message=f"Condition passed. Predicted confidence {prediction['confidence']} "
                    #    f"exceeds {condition_confidence}",
                    # )
                    ml_condition_flags.append(True)
                else:
                    # log_strategy(
                    #    item,
                    #    message=f"condition failed. predicted confidence {prediction['confidence']} "
                    #    f"does not exceed {condition_confidence}",
                    # )
                    ml_condition_flags.append(False)

        if not all(ml_condition_flags):
            condition_flag = False

    elif condition["type"] == "top_value_in":
        ml_condition_flags = []
        # log_strategy(item, "procssing conditions 'top_value_in'")
        condition_values = condition.get("value")
        for model in ml_predictions:
            # We can do this based on the current datstructure
            if field == model[0]["field"]:
                model_values = [value["value"] for value in model]
                # Check for matches between two list
                if bool(set(condition_values).intersection(model_values)):
                    # log_strategy(
                    #    item,
                    #    message=f"condition passed. found a matching a value from {model_values}"
                    #    f"in prediction",
                    # )
                    ml_condition_flags.append(True)
                else:
                    # log_strategy(
                    #    item,
                    #    message=f"condition failed. failed to find a matching a value from "
                    #    f"{model_values} in prediction",
                    # )
                    ml_condition_flags.append(False)

        condition_flag = check_condition_status(ml_condition_flags)

    elif condition["type"] == "top_value_not_in":
        ml_condition_flags = []
        # log_strategy(item, message="procssing conditions 'top_value_not_in'")
        condition_values = condition.get("value")
        for model in ml_predictions:
            # We can do this based on the current datstructure
            if field == model[0]["field"]:
                model_values = [value["value"] for value in model]
                # Check for matches between two list
                if not bool(set(condition_values).intersection(model_values)):
                    # log_strategy(
                    #    item,
                    #    message=f"Condtion passed. Failed to find a matching a value from "
                    #    f"{model_values} in prediction",
                    # )
                    ml_condition_flags.append(True)
                else:
                    # log_strategy(
                    #    item,
                    #    message=f"Condtion failed. Found a matching a value from {model_values} "
                    #    f"in prediction",
                    # )
                    ml_condition_flags.append(False)

        condition_flag = check_condition_status(ml_condition_flags)

    elif condition["type"] == "check_mailbox_status":
        condition_flag = False
        email_id = item["keywords"].get("email_id", "Unknown")

        # Get lookup dict from condition field
        lookup_dict = None

        if not lookup_dict:
            log_strategy(item, f"Unable to find lookup dict for: {field}")
            condition_flag = False
        else:
            mail_status = lookup_dict.get(email_id[0], "Offline")

            # Check if mailbox status is the same as set in condition. Eg Business Live
            if mail_status == condition["value"]:
                # log_strategy(
                #     item,
                #     message=f"Condition Passed. Email id: {email_id[0]} has value: '{condition['value']}' in lookup dictionary:'{condition['field']}'",
                # )
                condition_flag = True

            else:
                # log_strategy(
                #     item,
                #     message=f"Condition Failed. Email id: {email_id[0]} does not have value: '{condition['value']}' in lookup dictionary:'{condition['field']}",
                # )
                condition_flag = False

    else:
        log_strategy(
            item,
            message=f"condition: type {condition['type']} not recognised. setting flag to False",
        )
        condition_flag = False

    return condition_flag


def check_condition_status(ml_condition_flags):
    condition_flag = True
    if not all(ml_condition_flags):
        condition_flag = False
    return condition_flag


def check_top_confidence_value(field, condition_value, ml_predictions):
    """
    Check ml prediction specified has the highest confidence
    :return boolean
    """

    # log.error(f"Current ml predictions {ml_predictions}")
    # First get all results with field
    filtered_pred = []
    for row in ml_predictions:
        if isinstance(row, dict):
            if row["field"] == field:
                filtered_pred.append(row)

        elif isinstance(row, list):
            if row[0]["field"] == field:
                filtered_pred.append(row[0])

    confidence_flag = False

    # log.info(f"Filtered rows to perform predictions on {filtered_pred}")
    most_confident_row = max(filtered_pred, key=lambda x: x["confidence"])

    if most_confident_row["value"] == condition_value:
        confidence_flag = True
    return confidence_flag


def log_strategy(item=None, message=None, level="debug"):
    # Helper function to append ml_messages

    if level == "debug":
        print(message)
    elif level == "info":
        print(message)
    elif level == "warn":
        print(message)
    else:
        print(message)

    item["ml_messages"].append(message)
    return item


@dataclass
class MLStrategyVersion:
    name: str
    # status: str
    # profile: str
    strategy: list
    input_fields: field(default_factory=list)
    test_fields: field(default_factory=list)
    known_values: field(default_factory=dict)
    test_result: field(default_factory=dict)
    test_query: str = "*"
    test_limit: int = 1000
    test_project: str = "GEMS"
    version: int = 83

    created_at: str = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
    modified_at: str = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
    norm_mode: str = "alphanum"


@dataclass
class MLStrategy:
    name: str
    profile: str = "default"
    published_version: int = 0
    max_version: int = 0


@dataclass
class MLModel:
    name: str
    field: str
    source_project: str
    # training_stats: field(default_factory=dict)
    accepted_values: field(default_factory=list)
    ignored_values: field(default_factory=list)
    skipped_values: field(default_factory=list)
    aliases: field(default_factory=dict)
    custom_classifier_steps: field(default_factory=list)
    multi_value_ranges: field(default_factory=list)
    published_workflow_ids: field(default_factory=list)
    include_fields_in_training: field(default_factory=list)
    profile: str = "default"
    training_query: str = "dataset:training"
    validate_query: str = "dataset:test"
    training_limit: int = -1
    confidence_limit: int = 70
    min_word_count: int = 5
    published_version: int = 0
    published_workflow_id: str = ""
    max_version: int = 0
    validate_values: bool = True
    counterbalance_values: bool = True
    punish_low_training_counts: bool = False
    single_value_lower_limit: int = 0
    norm_mode: str = "alphanum"


def load_ml_strategy(profile, name, override_environment=None):
    with open(
        "strategies.json", "r"
    ) as file:
        data = json.load(file)
        ret = data[name]

    if not ret:
        raise Exception(f"failed to load strategy {name}")
    if name == "cash" or "trade":
        kwargs = {"name": "cash_productname"}
    else:
        kwargs = ret
    return MLStrategy(**kwargs)


def load_ml_strategy_version(profile, name, version, override_environment=None):
    print(f"loading {name} and {version}")
    with open(
        "strategies.json", "r"
    ) as file:
        data = json.load(file)
        ret = data[name]

    if not ret:
        raise Exception(f"failed to load strategy version {name} {version}")

    kwargs = ret

    if not kwargs.get("input_fields"):
        kwargs["input_fields"] = [
            "receiver_mailbox",
            "originating_country",
            "sender_domain",
            "sroriginatingcountry",
            "email_id",
            "productgroup",
            "productsubgroup",
        ]

    if not kwargs.get("test_fields"):
        kwargs["test_fields"] = []

    if not kwargs.get("known_values"):
        kwargs["known_values"] = {}

    if not kwargs.get("test_result"):
        kwargs["test_result"] = {}

    return MLStrategyVersion(**kwargs)


def load_ml_model(profile, name, override_environment=None):

    with open(
        "models.json", "r"
    ) as file:
        data = json.load(file)
    kwargs = data[name]

    for key in [
        "training_uppwer_limit",
        "training_upper_limit",
        "training_lower_limit",
        "mode",
        "balanced",
        "counter_balance",
        "retrain_only",
    ]:
        if kwargs.get(key) or key in kwargs:
            del kwargs[key]

    if not kwargs.get("custom_classifier_steps"):
        kwargs["custom_classifier_steps"] = []

    if not kwargs.get("multi_value_ranges"):
        kwargs["multi_value_ranges"] = []

    if not kwargs.get("published_workflow_ids"):
        kwargs["published_workflow_ids"] = []

    if not kwargs.get("include_fields_in_training"):
        kwargs["include_fields_in_training"] = []

    return MLModel(**kwargs)


def find_index_ml_predictions(field, ml_predictions):
    """
    Helper function to find model index/check if predictions exists
    :returns index
    """
    model_index = 0
    # If not predictions set starting pont to beginning
    # This might be misleading potentially update to check for nested empty lists
    if not ml_predictions:
        return model_index

    # Go through prediction
    for idx, modellist in enumerate(ml_predictions):
        # manually add one here for truth/falsy
        idx += 1
        if modellist:
            if field == modellist[0]["field"]:
                model_index = idx
                break

    return model_index


def predict_fasttext(
    workflow_id,
    target_keyword,
    items,
    k=1,
    inference_folder="/opt/sq_mig/models",
):
    """
    Version that used models direclty built using fasttext auto tuning
    """

    # create the absolute model path
    model_file = os.path.join(
        inference_folder,
        workflow_id,
        "fasttext.bin",
    )

    # evict cache entries older than 1 hour
    timestamp = int(time.time())
    cache_hit = MODEL_CACHE.get(model_file)

    if cache_hit:
        # log.info(f"CACHE HIT: {workflow_id}")
        if timestamp - cache_hit["ts"] < 3600:
            model = cache_hit["model"]
        else:
            cache_hit = None

    if not cache_hit:
        # log.info(f"CACHE MISS: {workflow_id}")
        model = fasttext.load_model(model_file)
        MODEL_CACHE[model_file] = {"model": model, "ts": timestamp}

        entries_to_evict = []
        for key, value in MODEL_CACHE.items():
            created_ts = value["ts"]
            if timestamp - created_ts > 3600:
                entries_to_evict.append(key)

        for entry in entries_to_evict:
            # log.info(f"Cache eviction: {entry}")
            del MODEL_CACHE[entry]

    target_kw = f"pred_{target_keyword}"

    result_items = []

    for input_item in items.get("items"):
        result_item = {
            "id": input_item.get("id"),
            "body": input_item.get("body"),
            "keywords": {},
        }

        labels, scores = model.predict(input_item["body"], k=k)

        predictions = []

        for index, label in enumerate(labels):
            clean_label = label.replace("__label__", "")

            # recover the label from bas64
            clean_label = base64.b64decode(clean_label).decode("utf-8")

            predictions.append({clean_label: scores[index]})

        result_item["keywords"][target_kw] = predictions
        result_items.append(result_item)

    return {"items": result_items}


def field_quorum_winner(item, global_variables_dict, kwargs=None):
    """
    Custom step that ensure that best pred is returned based on quorum ranking
    Return the value predicted by most models, if there is a tie > 1 return the one with the most
    training docs, else if only one left, lets use the the one with lowest training docs but highest confidence
    """

    # Early exit if no predictions or no field specified
    if not item.get("ml_predictions"):
        log_strategy(item, message="No ml_predictions, returning item...")
        return item

    field = kwargs.get("field")
    if not field:
        log_strategy(item, message="Unable to find field in kwargs")
        return item

    limit = kwargs.get("limit", 3)

    # Find model index
    model_index = find_index_ml_predictions(field, item["ml_predictions"])

    if not model_index:
        log_strategy(
            item, message=f"Unable to find ML results for {field}. Returning item..."
        )
        return item

    # Manually reduce to -1. Possibly split up functions.
    # To split up find_index_ml_preictions()
    model_index -= 1

    field_predictions = item["ml_predictions"][model_index]
    winning_predictions = []
    winning_values = []
    count = 0

    # first we re-order all prediction by predicted values
    value_dict = defaultdict(list)

    for prediction in field_predictions:
        value_dict[prediction["value"]].append(prediction)

    while len(winning_predictions) < limit and count < limit and len(value_dict):
        count += 1

        # establish the predictions with the most predictions
        max_count = 0

        for field, predictions in value_dict.items():
            if len(predictions) > max_count:
                max_count = len(predictions)

        # filter out the top predictions
        top_predictions = []
        for field, predictions in value_dict.items():
            if len(predictions) == max_count:
                top_predictions.append(predictions)

        if len(top_predictions) == 1:
            # only one"" winner, we're done
            log_strategy(
                item, f"found a single top prediction with {max_count} predictions"
            )
            winning_predictions.append(top_predictions[0][0])
            del value_dict[top_predictions[0][0]["value"]]
            winning_values.append(top_predictions[0][0]["value"])
            continue

        else:
            log_strategy(
                item,
                f"found {len(top_predictions)} predictions with {max_count} predictions",
            )

            top_confidence = 0.0
            for predictions in top_predictions:
                sum_confidence = 0.0

                for prediction in predictions:
                    sum_confidence += prediction["confidence"]

                if sum_confidence > top_confidence:
                    winning_prediction = [predictions[0]]
                    top_confidence = sum_confidence

            winning_predictions.append(winning_prediction[0])
            del value_dict[winning_prediction[0]["value"]]
            winning_values.append(winning_prediction[0]["value"])

    log_strategy(item, message=f"Winning predictions are {winning_values}")

    # Overwrite model preidctions with
    item["ml_predictions"][model_index] = winning_predictions

    return item


def set_min_confidence(item, global_variables_dict):
    # needed for the api with pega to function
    # In the past the prediction would emmit low confidence prediction that pega then discards
    # The current version of the prediction already takes care of this, so for pega not to discard
    # anything, we floor all predictions at 0.8

    for predictions in item["ml_predictions"]:
        for prediction in predictions:
            if prediction.get("confidence") < 0.8:
                prediction["confidence"] = 0.8
    return item


def refresh_picklist(global_variables_dict):
    cutoff = datetime.now() - timedelta(hours=24)
    if global_variables_dict.get("last_picklist_refresh") < cutoff:
        refresh = True
    else:
        refresh = False

    # refresh picklist data if needed
    if not global_variables_dict.get("gems_ml_combinations") or refresh:
        print("Picklist data empty or outdated")
        with open(
            f"picklist_values.json",
            "r",
        ) as f:
            rows = json.load(f)
            for row in rows:
                keys = generate_picklist_keys(row)
                # log.info(f"Generated following keys {keys}")
                for key in keys:
                    global_variables_dict["gems_ml_combinations"].append(key)

                # generate productname -> productsubgroup lookup dict
                if row["productgroup"] == "Trade":
                    global_variables_dict["trade_productsubgroup_lookup"][
                        row["productname"]
                    ] = row["productsubgroup"]
                    # print(
                    #     f"Added trade lookup {row['productname']} -> {row['productsubgroup']}"
                    # )

            f.close()

        global_variables_dict["last_picklist_refresh"] = datetime.now()


def set_trade_productsubgroup(item, global_variables_dict):
    refresh_picklist(global_variables_dict)

    productsubgroup_predictions = []
    seen_values = []

    # find the productname predictions
    for predictions in item["ml_predictions"]:
        for prediction in predictions:
            if prediction.get("field") == "productname":
                # get the matching productsubgroup

                if prediction.get("confidence") < 0.10:
                    continue

                value = global_variables_dict["trade_productsubgroup_lookup"].get(
                    prediction.get("value")
                )

                if value in seen_values:
                    continue

                log_strategy(
                    item,
                    f"set trade subgroup: productname {prediction.get('value')} -> productsubgroup: {value}",
                )

                productsubgroup_predictions.append(
                    {
                        "value": value,
                        "field": "productsubgroup",
                        "confidence": prediction.get("confidence"),
                        "training_docs": prediction.get("training_docs"),
                        "model": "custom_set_trade_productsubgroup",
                        "model_version": 1,
                    }
                )

                seen_values.append(value)

    if productsubgroup_predictions:
        item["ml_predictions"].append(productsubgroup_predictions)

    return item


def generate_picklist_keys(row, model_combination_set=[]):
    """
    Generate various keys with __NONE__ values for subproduct, productservice, querytype
    Product field should always be known
    :returns keys for Redis
    """
    # To be imported from strategy
    key_postions = {
        1: "productgroup",
        2: "productsubgroup",
        3: "productname",
        4: "querytype",
    }

    # When building items from json, all fields are immediately available
    if isinstance(row, dict):
        # First build full key
        full_key = f"{row[key_postions[1]]}|{row[key_postions[2]]}|{row[key_postions[3]]}|{row[key_postions[4]]}"
        missing_key1 = f"{row[key_postions[1]]}|{row[key_postions[2]]}|{row[key_postions[3]]}|__NONE__"
        missing_key2 = (
            f"{row[key_postions[1]]}|{row[key_postions[2]]}|__NONE__|__NONE__"
        )
        missing_key3 = f"{row[key_postions[1]]}|__NONE__|__NONE__|__NONE__"
        # print(
        #     f"Found following keys when building JSON {full_key}, {missing_key1}, {missing_key2}, {missing_key3}"
        # )

    else:
        new_comb = {}
        for model in model_combination_set:
            new_comb[model] = {}
            for comb in row:
                if model == comb["field"]:
                    new_comb[model].update(comb)

        # log.info(f"New comb: {new_comb} as follows")

        full_key = f'{new_comb.get(key_postions[1]).get("value", "unkown")}|{new_comb.get(key_postions[2]).get("value", "unkown")}|{new_comb.get(key_postions[3]).get("value", "unkown")}|{new_comb.get(key_postions[4]).get("value", "unkown")}'
        missing_key1 = f'{new_comb.get(key_postions[1]).get("value", "unkown")}|{new_comb.get(key_postions[2]).get("value", "unkown")}|{new_comb.get(key_postions[3]).get("value", "unkown")}|__NONE__'
        missing_key2 = f'{new_comb.get(key_postions[1]).get("value", "unkown")}|{new_comb.get(key_postions[2]).get("value", "unkown")}|__NONE__|__NONE__'
        missing_key3 = f'{new_comb.get(key_postions[1]).get("value", "unkown")}|__NONE__|__NONE__|__NONE__'

        print(
            f"Found following keys when processing results {full_key}, {missing_key1}, {missing_key2}, {missing_key3}"
        )

    return [full_key, missing_key1, missing_key2, missing_key3]


def evaluate_picklist(item, global_variables_dict):
    refresh_picklist(global_variables_dict)

    # To be configurable from ini file/strategy
    model_combination_set = [
        "productgroup",
        "productsubgroup",
        "productname",
        "querytype",
    ]

    pred_combinations = list(itertools.product(*item["ml_predictions"]))

    # List to generate as many keys as possible
    generated_keys = []

    processed_combinations = defaultdict()

    for combination in pred_combinations:
        generated_keys.extend(
            generate_picklist_keys(combination, model_combination_set)
        )
        for key in generated_keys:
            if (
                key in global_variables_dict.get("gems_ml_combinations")
                and key not in processed_combinations
            ):
                # log_strategy(item, message=f"Valid combo: {key} in picklist_set")
                filtered_combinations = filter_combinations(key, combination)
                processed_combinations[key] = filtered_combinations
            else:
                # print(f"Unable to find key: {key} in picklist_set")
                pass

    if len(processed_combinations):
        # find best combiantion
        winning_combination = calculate_best_performing_combinations(
            item, processed_combinations
        )

        item["ml_predictions"] = winning_combination
    else:
        log_strategy(
            item,
            message="- No valid combo found",
        )

        item["ml_predictions"] = []

    return item


def filter_combinations(key, combination):
    """
    Filter combinations based of strategy key
    Ex: Trade|__NONE__|__NONE__|__NONE__ should only return Trade results
    :returns nsted list of combinations
    """
    combinations_to_return = []

    # Get values to split
    key_values = key.split("|")
    for combo in combination:
        for value in key_values:
            # to remove hardcoded value
            if combo["value"] == value:
                combinations_to_return.append([combo])
    return combinations_to_return


def calculate_best_performing_combinations(item, combinations):
    """
    Process combinations
    :return best performing strategy
    """
    total_confidence_score = 0
    total_training_docs = 0

    best_combo = ()

    # Calculate total confidence
    for comb_key, comb_values in combinations.items():
        # log_strategy(item, message=f"Updated comb values are {filtered_comb_values}")
        combination_confidence_score = find_top_value_combination(
            "confidence", comb_values
        )
        combination_training_docs = find_top_value_combination(
            "training_docs", comb_values
        )
        log_strategy(
            item,
            message=f"- Combo {comb_key}: "
            f"conf:{round(combination_confidence_score, 4)} docs:{combination_training_docs}",
        )

        # Check the highest score
        if combination_confidence_score > total_confidence_score:
            total_confidence_score = combination_confidence_score
            total_training_docs = combination_training_docs
            best_combo = (
                comb_key,
                total_confidence_score,
                total_training_docs,
                combinations[comb_key],
            )

        # If combination scores are the take the one with the highest training docs
        elif combination_confidence_score == total_confidence_score:
            if combination_training_docs > total_training_docs:
                total_confidence_score = combination_confidence_score
                total_training_docs = combination_training_docs

                best_combo = (
                    comb_key,
                    total_confidence_score,
                    total_training_docs,
                    combinations[comb_key],
                )

    log_strategy(
        item,
        message=f"- Winner '{best_combo[0]}' "
        f"conf: {round(best_combo[1], 4)} docs: {best_combo[2]}",
    )
    return best_combo[3]


def find_top_value_combination(field, combination):
    """
    Process nested list of dicts
    :return sum of all prediction confidences
    """

    if field == "confidence":
        total = sum(pred[0][field] for pred in combination)
        total += total / len(combination)
        return total
    else:
        return sum(pred[0][field] for pred in combination)


def field_quorum_winner_casedetach(item, global_variables_dict, kwargs=None):
    """
    Custom step that ensure that best pred is returned based on quorum ranking
    Return the value predicted by most models, if there is a tie > 1 return the one with the most
    training docs, else if only one left, lets use the the one with lowest training docs but highest confidence
    """
    # Early exit if no predictions or no field specified
    if not item.get("ml_predictions"):
        log_strategy(item, message="No ml_predictions, returning item...")
        return item

    field = kwargs.get("field")
    if not field:
        log_strategy(item, message="Unable to find field in kwargs")
        return item

    if field != "is_case_detach":
        log_strategy(
            item,
            message="'field_quorum_winner_casetat' step only applicable for is_case_detach field. Use field_quorum_winner instead...",
        )
        return item

    # hardcode limit since we only want 1 result back
    limit = 1

    # Find model index
    model_index = find_index_ml_predictions(field, item["ml_predictions"])

    if not model_index:
        log_strategy(
            item, message=f"Unable to find ML results for {field}. Returning item..."
        )
        return item

    # Manually reduce to -1. Possibly split up functions.
    # To split up find_index_ml_preictions()
    model_index -= 1

    field_predictions = item["ml_predictions"][model_index]
    winning_predictions = []
    winning_values = []
    count = 0

    # first we re-order all prediction by predicted values
    value_dict = defaultdict(list)

    for prediction in field_predictions:
        value_dict[prediction["value"]].append(prediction)

    while len(winning_predictions) < limit and count < limit and len(value_dict):
        count += 1

        # establish the predictions with the most predictions
        max_count = 0

        for field, predictions in value_dict.items():
            if len(predictions) > max_count:
                max_count = len(predictions)

        # filter out the top predictions
        top_predictions = []
        for field, predictions in value_dict.items():
            if len(predictions) == max_count:
                top_predictions.append(predictions)

        if len(top_predictions) == 1:
            # only one"" winner, we're done
            log_strategy(
                item, f"found a single top prediction with {max_count} predictions"
            )
            winning_predictions.append(top_predictions[0][0])
            del value_dict[top_predictions[0][0]["value"]]
            winning_values.append(top_predictions[0][0]["value"])
            continue

        else:
            log_strategy(
                item,
                f"found {len(top_predictions)} predictions with {max_count} predictions",
            )

            top_confidence = 0.0
            for predictions in top_predictions:
                sum_confidence = 0.0

                for prediction in predictions:
                    sum_confidence += prediction["confidence"]

                if sum_confidence > top_confidence:
                    winning_prediction = [predictions[0]]
                    top_confidence = sum_confidence

            winning_predictions.append(winning_prediction[0])
            del value_dict[winning_prediction[0]["value"]]
            winning_values.append(winning_prediction[0]["value"])

    log_strategy(item, message=f"Winning predictions are {winning_values}")

    # Overwrite model preidctions with
    item["ml_predictions"][model_index] = winning_predictions

    return item


def process_isquickkill_complexity(item, global_variables_dict, kwargs=None):
    """
    Custom step to determine whether isquickkill fields should be set as 'Not Predicted'
    based on confidence values.
    """
    # Early exit if no predictions or no field specified
    if not item.get("ml_predictions"):
        log_strategy(item, message="- No ml_predictions, returning item...")
        return item

    field = kwargs.get("field")
    confidence = kwargs.get("confidence", 0.8)
    delta = kwargs.get("delta", 0.2)

    if not field:
        log_strategy(item, message="Unable to find field in kwargs")
        return item

    if (
        field != "isquickkill"
        and field != "isquickkill_corrected"
        and field != "is_l2"
        and field != "is_case_detach"
    ):
        log_strategy(
            item,
            message="- 'isquickkill'/'isquickkill_corrected'/'is_l2/is_case_detach' are the only supported fields to process complexity. Returning item...",
        )
        return item

    quickkill_model_index = find_index_ml_predictions(field, item["ml_predictions"])
    if not quickkill_model_index:
        log_strategy(
            item, message=f"- No ml predictions for {field}, returning item..."
        )
        return item

    # for prediction in item["ml_predictions"][field_predictions][quickkill_model_index]:
    quickkill_model_index -= 1

    quickkill_predictions = item["ml_predictions"][quickkill_model_index]

    if len(quickkill_predictions) == 1:
        if quickkill_predictions[0]["confidence"] < confidence:
            log_strategy(
                item,
                message=f"- Only one prediction found for issquickkill with confidence less than {confidence}. Setting to prediction to 'not set'.",
            )
            item["ml_predictions"][quickkill_model_index][0]["value"] = ["not set"]
        else:
            log_strategy(
                item,
                message=f"- Only one prediction found for isquickkill with confidence higher than {confidence}. Keeping original prediction value of: {quickkill_predictions[0]['confidence']} ",
            )

    else:
        log_strategy(
            item,
            message=f"- Multiple predictions found for {field}.. Calculating winner and loser...",
        )
        # Define winner/loser combinations
        winner = max(quickkill_predictions, key=lambda x: x["confidence"])
        loser = min(quickkill_predictions, key=lambda x: x["confidence"])
        # how many above 80, if 1 good

        # now find out if we return winner
        if winner["confidence"] >= confidence:
            conf_delta = winner["confidence"] - loser["confidence"]
            if conf_delta > delta and winner["value"] != loser["value"]:
                log_strategy(
                    item,
                    message=f"- Delta larger than {delta} for {field}. Setting prediction to 'not set'",
                )
                winner["value"] = ["not set"]
                item["ml_predictions"][quickkill_model_index] = [winner]

            else:
                log_strategy(
                    item,
                    message=f"- Delta less than {delta} for {field}. Setting prediction to '{winner['value']}'",
                )
                item["ml_predictions"][quickkill_model_index] = [winner]

        else:
            # Overwrite predictions
            winner["value"] = ["not set"]
            item["ml_predictions"][quickkill_model_index] = [winner]
            log_strategy(
                item,
                message=f"- Winner is less than {confidence}. Setting prediction value to 'not set'",
            )

    return item


if __name__ == "__main__":
    # predict("default")
    app.run(host="0.0.0.0")
