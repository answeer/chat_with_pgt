class HarmfulString(Scanner):
    """
    A class to detect and optionally sanitize harmful strings in text data.

    This class extends the `Scanner` base class to identify specific harmful strings
    from a configuration file within given text data. It can optionally sanitize the text
    by replacing harmful strings with a placeholder.

    Attributes:
        sanitize (bool): Indicates whether to sanitize (redact) harmful strings from the text.
        case_sensitive (bool): Indicates whether the search for harmful strings should be case-sensitive.
        harm_str (dict): A dictionary of harmful strings loaded from a configuration file.

    Methods:
        redact_text(text, substrings):
            Redacts (replaces with "[REDACTED]") harmful substrings in the given text.

        predict(data):
            Detects harmful strings in the provided text and optionally sanitizes it. Returns the
            result of the detection, score, and sanitized text.

        format_response():
            Formats the response to include the prediction result, score, and sanitized text.
    """

    def __init__(self, **kwargs):
        """
        Initializes the HarmfulString instance.

        Args:
            **kwargs: Additional keyword arguments, including 'sanitize' to determine if harmful strings
                      should be redacted and 'case_sensitive' to control case sensitivity of the search.
        """
        sanitize = kwargs["sanitize"]
        case_sensitive = kwargs['case_sensitive']
        super().__init__("harmful_string", 0.5, sanitize=sanitize, case_sensitive=case_sensitive)
        self.harm_str = read_config("harmful_strings.yml")

    def redact_text(self, text, substrings):
        """
        Redacts harmful substrings from the given text by replacing them with "[REDACTED]".

        Args:
            text (str): The text in which harmful substrings are to be redacted.
            substrings (list): A list of harmful substrings to be redacted.

        Returns:
            str: The text with harmful substrings redacted.
        """
        redacted_text = text
        for s in substrings:
            redacted_text = redacted_text.replace(s, "[REDACTED]")
        return redacted_text

    def predict(self, data):
        """
        Detects harmful strings in the provided text and optionally sanitizes it.

        Args:
            data (str): The text data to be checked for harmful strings.

        Returns:
            tuple: A tuple containing:
                - A message indicating the presence of harmful strings or lack thereof.
                - A score indicating the result (1 for no harmful strings, 0 for detected harmful strings).
                - The sanitized text if harmful strings were detected and sanitization is enabled.
        """
        sanitized_data = data
        matched_substrings = []
        missing_substrings = []
        try:
            for s in self.harm_str['harmful_strings']:
                if not self._kwargs['case_sensitive']:
                    s, data = s.lower(), data.lower()

                if s in data:
                    matched_substrings.append(s)
                else:
                    missing_substrings.append(s)

            if matched_substrings:
                if self._kwargs['sanitize']:
                    sanitized_data = self.redact_text(sanitized_data, matched_substrings)
                predict = "Found the harmful strings: {}.".format(', '.join(matched_substrings))
                score = 0
            else:
                predict = "No harmful strings found."
                score = 1
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, e)
            predict = "Error occurred: {}".format(e)
            score = 0

        return predict, score, sanitized_data

    def format_response(self):
        """
        Formats the prediction response to include the detection result, score, and sanitized text.

        This method updates the response dictionary to include:
            - The prediction result under the "harmful_string" key.
            - The score under the "score" key.
            - The sanitized data under the "sanitized_data" key.
        """
        self.response["prediction"]["harmful_string"] = self.pred[0]
        self.response["score"] = self.pred[1]
        self.response['sanitized_data'] = self.pred[2]
