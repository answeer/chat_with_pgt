Cross-entropy loss measures the difference between the predicted probability distribution (softmax output) and the true label distribution (one-hot encoding of the actual word/token).

Formula:

\text{Loss} = -\sum_{t=1}^{T} \log(p(y_t | y_{<t}, x))

where:

	•	 T  is the length of the output sequence.
	•	 y_t  is the actual token at position  t .
	•	 p(y_t | y_{<t}, x)  is the predicted probability for the correct token given the previous tokens and the input sequence  x .

This approach trains the Transformer to maximize the probability of the correct sequence.
