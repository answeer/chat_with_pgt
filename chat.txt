from llm_sanitation.scanners.scanner_base import Scanner
from llm_sanitation.utils.models import *
from llm_sanitation.utils.common import read_config
from llm_sanitation.logging.logging_setup import LogUtil, LogType, LogLevel

class HarmfulString(Scanner):

    def __init__(self, **kwargs):
        sanitize = kwargs["sanitize"]
        case_sensitive = kwargs['case_sensitive']
        super().__init__("harmful_stgring", 0.5,sanitize=sanitize,case_sensitive=case_sensitive)
        self.harm_str = read_config("harmful_strings.yml")

    def redact_text(self,text,substrings):
        redacted_text = text
        for s in substrings:
            redacted_text = redacted_text.replace(s, "[REDACTED]")
        return redacted_text

    def predict(self, data):
        sanitized_data = data
        matched_substrings = []
        missing_substrings = []
        try:
            for s in self.harm_str['harmful_strings']:
                if self._kwargs['case_sensitive'] is False:
                    s, data = s.lower(), data.lower()

                if s in data:
                    matched_substrings.append(s)
                else:
                    missing_substrings.append(s)

            if matched_substrings:

                if self._kwargs['sanitize']:
                    sanitized_data = self.redact_text(sanitized_data, matched_substrings)
                    predict = "Found the harmful strings: {}.".format(', '.join(matched_substrings))
                    score = 0
            else:
                predict = "No harmful strings found."
                score = 1
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, e)
            predict = "Error occured: {}".format(e)
            score = 0

        return predict, score, sanitized_data
 
    def format_response(self):
        self.response["prediction"]["harmful_stgring"] = self.pred[0]
        self.response["score"] = self.pred[1]
        self.response['sanitized_data'] = self.pred[2]
