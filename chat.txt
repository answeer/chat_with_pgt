\subsection{Training Loss}
The student is trained with a distillation loss over the soft target probabilities of the teacher:
\[
L_{\text{ce}} = \sum_i t_i \log(s_i)
\]
where \( t_i \) (resp. \( s_i \)) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al. [2015], we used a softmax-temperature:
\[
p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\]
where \( T \) controls the smoothness of the output distribution and \( z_i \) is the model score for the class \( i \). The same temperature \( T \) is applied to the student and the teacher at training time, while at inference, \( T \) is set to 1 to recover a standard softmax.

The final training objective is a linear combination of the distillation loss \( L_{\text{ce}} \) with the supervised training loss, in our case the masked language modeling loss \( L_{\text{mlm}} \) [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (\( L_{\text{cos}} \)) which will tend to align the directions of the student and teacher hidden states vectors.
\]
