import profanity_check
from llm_sanitation.scanners.scanner_base import Scanner
from llm_sanitation.utils.models import *
import nltk
from llm_sanitation.logging.logging_setup import LogUtil, LogType, LogLevel

class ProfanityCheck(Scanner):

    def __init__(self, **kwargs):
        super().__init__("profanity", 0.5)

    def predict(self, data):
        # sanitized_text = ""
        try:
            predict = profanity_check.predict([data])
            score = profanity_check.predict_prob([data])
            if predict[0] == 1:
                predict = True
            else:
                predict = False
            # if self._kwargs["sanitize"]:
            #     sentences = nltk.sent_tokenize(data)
            #     supported_sentences = []
            #     for sentence in sentences:
            #         predict = profanity_check.predict([sentence])
            #         if not predict:
            #             supported_sentences.append(sentence)
            #         sanitized_text="\n".join(supported_sentences)
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, e)
            predict = "Error occured: {}".format(e)
            score = 0
        return predict,score[0]

    def format_response(self):
        self.response["prediction"]["Contain_profanity"] = self.pred[0]
        self.response["score"] = 1 - self.pred[1]
        # self.response['sanitized_text'] = self.pred[2]
