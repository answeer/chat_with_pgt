from llm_sanitation.scanners.scanner_base import Scanner
from llm_sanitation.utils.models import *
from presidio_analyzer.nlp_engine import NlpEngineProvider
from presidio_analyzer.analyzer_engine import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer
from presidio_analyzer import (
    AnalyzerEngine,
    Pattern,
    PatternRecognizer,
    RecognizerRegistry,
)
import spacy
import copy
from llm_sanitation.utils.common import read_config
from llm_sanitation.logging.logging_setup import LogUtil, LogType, LogLevel

class PiiDetector(Scanner):

    def __init__(self, **kwargs):
        sanitize=kwargs["sanitize"]
        super().__init__("pii_detector", 0.5, sanitize=sanitize)
        self.read_config()
        self.get_analyzer()

    def read_config(self):
        config = read_config("pii_detector.yml")
        self.default_entities = config['default_entity_types']
        self.default_regex_patterns = config['default_regex_patterns']

    def get_regex_patterns(self,regex_patterns):
        result = []
        for group in regex_patterns:
            result.append(
                {
                    "name": group["name"].upper(),
                    "expressions": group.get("expressions", []),
                    "context": group.get("context", []),
                    "score": group.get("score", 0.75),
                    "languages": group.get("languages", ["en"]),
                    "reuse": group.get("reuse", False),
                }
            )
        return result

    def add_recognizers(self,registry,regex_groups):
        """
        Create a RecognizerRegistry and populate it with regex patterns and custom names.

        Parameters:
            regex_groups: List of regex patterns.

        Returns:
            RecognizerRegistry: A RecognizerRegistry object loaded with regex and custom name recognizers.
        """

        for pattern_data in regex_groups:

            label = pattern_data["name"]
            reuse = pattern_data.get("reuse", False)

            patterns = map(
                lambda exp: Pattern(name=label, regex=exp, score=pattern_data["score"]),
                pattern_data.get("expressions", []) or [],
            )


            if reuse:
                new_recognizer = copy.deepcopy(
                    registry.get_recognizers(language=reuse["language"], entities=[reuse["name"]])[
                        0
                    ]
                )
                registry.add_recognizer(new_recognizer)
            else:
                registry.add_recognizer(
                    PatternRecognizer(
                        supported_entity=label,
                        patterns=patterns,
                        context=pattern_data["context"],
                    )
                )

        return registry

    def get_analyzer(self):
        try:
            # Use small spacy model, for faster inference.
            if not spacy.util.is_package("en_core_web_sm"):
                spacy.cli.download("en_core_web_sm")

            nlp_configuration = {
                "nlp_engine_name": "spacy",
                "models": [{"lang_code": "en", "model_name": "en_core_web_sm"}],
            }
            nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()
            regex_groups = self.get_regex_patterns(self.default_regex_patterns)
            registry = RecognizerRegistry()
            registry.load_predefined_recognizers(nlp_engine=nlp_engine)
            registry = self.add_recognizers(registry, regex_groups)
            registry.remove_recognizer("SpacyRecognizer")
            self.analyzer = AnalyzerEngine(registry=registry,
                                        nlp_engine=nlp_engine,
                                        context_aware_enhancer=LemmaContextAwareEnhancer(
                                        context_similarity_factor=0.35,
                                        min_score_with_context_similarity=0.4,))
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, e)

    def predict(self,data):
        sanitized_prompt = data
        try:
            analyzer_results = self.analyzer.analyze(
                text=data.replace("'", " "),
                language="en",
                entities=self.default_entities,
                score_threshold=self.score_thresh,
            )
            risk_score = round(
                (
                    max(analyzer_result.score for analyzer_result in analyzer_results)
                    if analyzer_results
                    else 0.0
                ),
                2,
            )
            pii_anonymizer = AnonymizerEngine()
            anonymized_text = pii_anonymizer.anonymize(
                text=data, analyzer_results=analyzer_results
            ).text
            if len(analyzer_results) is not None:
                predict = "PII detected."
            else:
                predict = "NO PII detected."
            if self._kwargs['sanitize']:
                sanitized_prompt = anonymized_text
            else:
                sanitized_prompt = data
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, e)
            predict = "Error occured: {}".format(e)
            risk_score = 1
        return predict,risk_score,sanitized_prompt

    def format_response(self):
        self.response["prediction"]["pii_detector"] = self.pred[0]
        self.response["score"] = 1 - self.pred[1]
        self.response['sanitized_data'] = self.pred[2]
