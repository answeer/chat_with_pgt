    def plot_token_accuracy(self, results, save_path='token_accuracy_comparison.png'):
        # 定义 token 区间
        token_ranges = [(0, 50), (51, 100), (101, 150), (151, 200), (201, 300), (301, 500), (501, float('inf'))]
        range_labels = [f'{start}-{int(end) if end != float("inf") else "inf"}' for start, end in token_ranges]

        # 准备绘图
        plt.figure(figsize=(12, 8))

        for model_nm, (token_counts, true_labels, predictions) in results.items():
            accuracies = []

            # 计算每个 token 区间的精度
            for start, end in token_ranges:
                indices = [i for i, tokens in enumerate(token_counts) if start <= tokens < end]
                if not indices:
                    accuracies.append(0)
                    continue
                
                range_true_labels = [true_labels[i] for i in indices]
                range_predictions = [predictions[i] for i in indices]
                accuracy = accuracy_score(range_true_labels, range_predictions)
                accuracies.append(accuracy)

            # 绘制该模型的精度曲线
            plt.plot(range_labels, accuracies, marker='o', label=model_nm)

        # 图形设置
        plt.xlabel('Token Range')
        plt.ylabel('Accuracy')
        plt.title('Token Range vs Accuracy Comparison for Models')
        plt.legend(title='Model')
        plt.savefig(save_path)
        plt.show()
        print(f"Token accuracy comparison plot saved to {save_path}")


if __name__ == "__main__":
    model_list = ['minicheck', 'hhem', 'factual_consistency']
    eval_dataset = EvalDataset()
    sample_ratio = 0.1  # 设置样本比例

    # 运行每个模型并收集结果
    results = {}
    for model_nm in model_list:
        print(f"Running evaluation for {model_nm}...")
        token_counts, true_labels, predictions = eval_dataset.run(model_nm, sample_ratio=sample_ratio)
        results[model_nm] = (token_counts, true_labels, predictions)

    # 绘制 token 数与精度的关系图
    eval_dataset.plot_token_accuracy(results)
