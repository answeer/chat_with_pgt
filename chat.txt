import pyarrow.parquet as pp
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import time


class EvalDataset:
    def __init__(self):
        self.df = None
        self.true_labels = None

    def load_datasets(self, dataset_paths):
        """
        加载多个数据集。
        :param dataset_paths: 数据集文件路径列表
        :return: 数据集字典，键为数据集名称，值为数据
        """
        datasets = {}
        for path in dataset_paths:
            dataset_name = path.split("\\")[-1].replace('.parquet', '')  # 根据文件名命名数据集
            parquet_file = pp.ParquetFile(path)
            data = parquet_file.read().to_pandas()
            datasets[dataset_name] = data
        return datasets

    def run_on_datasets(self, model_list, dataset_paths):
        """
        在多个数据集上运行评估。
        :param model_list: 模型名称列表
        :param dataset_paths: 数据集路径列表
        :return: 所有结果的嵌套字典
        """
        datasets = self.load_datasets(dataset_paths)
        all_results = {}

        for dataset_name, dataset in datasets.items():
            print(f"Running evaluation on dataset: {dataset_name}")
            self.df = dataset  # 更新当前数据集
            self.true_labels = dataset['label'].tolist()

            dataset_results = {}
            for model_nm in model_list:
                print(f"Evaluating model: {model_nm} on dataset: {dataset_name}")
                inference_times, token_counts, predictions, scores = self.run(model_nm)
                dataset_results[model_nm] = (inference_times, token_counts, predictions, scores)

            all_results[dataset_name] = dataset_results

        return all_results

    def run(self, model_nm):
        """
        模拟模型运行的过程（需要替换为实际推理代码）。
        :param model_nm: 模型名称
        :return: 推理时间、token数量、预测结果、分数
        """
        print(f"Simulating model {model_nm}...")
        start_time = time.time()

        # 模拟数据
        num_samples = len(self.df)
        token_counts = np.random.randint(10, 200, size=num_samples)
        predictions = np.random.randint(0, 2, size=num_samples)
        scores = np.random.random(size=num_samples)

        inference_time = time.time() - start_time
        inference_times = [inference_time] * num_samples  # 假设每个样本的推理时间一致

        return inference_times, token_counts, predictions, scores

    def save_results_to_excel(self, results, save_path):
        """
        将模型评估结果保存到 Excel 文件。
        :param results: 模型评估结果
        :param save_path: 保存路径
        """
        with pd.ExcelWriter(save_path) as writer:
            for model_name, (inference_times, token_counts, predictions, scores) in results.items():
                df = pd.DataFrame({
                    'Inference Time': inference_times,
                    'Token Count': token_counts,
                    'Prediction': predictions,
                    'Score': scores
                })
                df.to_excel(writer, index=False, sheet_name=model_name)
        print(f"Results saved to {save_path}")

    def compare_models_across_datasets(self, all_results, metric='accuracy', save_path='model_comparison_across_datasets.png'):
        """
        绘制跨数据集的模型性能比较图。
        :param all_results: 所有数据集的结果
        :param metric: 比较指标，默认为 accuracy
        :param save_path: 图像保存路径
        """
        dataset_names = list(all_results.keys())
        model_names = list(all_results[dataset_names[0]].keys())

        # 准备数据结构存储指标
        metric_values = {model: [] for model in model_names}

        for dataset_name, dataset_results in all_results.items():
            for model_name, (_, _, predictions, _) in dataset_results.items():
                accuracy = accuracy_score(self.true_labels, predictions)  # 可换为其他指标
                metric_values[model_name].append(accuracy)

        # 绘制柱状图
        bar_width = 0.2
        x = np.arange(len(dataset_names))
        plt.figure(figsize=(12, 8))

        for i, model_name in enumerate(model_names):
            plt.bar(x + i * bar_width, metric_values[model_name], bar_width, label=model_name)

        plt.xlabel('Datasets')
        plt.ylabel(metric.capitalize())
        plt.title(f'{metric.capitalize()} Comparison Across Datasets')
        plt.xticks(x + bar_width * (len(model_names) - 1) / 2, dataset_names, rotation=45)
        plt.legend(title='Models')
        plt.tight_layout()
        plt.savefig(save_path)
        plt.show()
        print(f"Model comparison plot saved to {save_path}")


if __name__ == "__main__":
    # 多个数据集路径
    dataset_paths = [
        'prompt_injection\\dataset\\safe-guard-prompt-injection\\train-00000-of-00001.parquet',
        'prompt_injection\\dataset\\another-dataset\\train.parquet',
        'prompt_injection\\dataset\\third-dataset\\train.parquet',
    ]

    # 模型列表
    model_list = ['deberta', 'savant_large', 'setfit']

    # 实例化并运行评估
    eval_dataset = EvalDataset()
    all_results = eval_dataset.run_on_datasets(model_list, dataset_paths)

    # 保存推理结果到 Excel 文件
    for dataset_name, results in all_results.items():
        file_name = f'model_inference_results_{dataset_name}.xlsx'
        eval_dataset.save_results_to_excel(results, file_name)

    # 绘制跨数据集比较图
    eval_dataset.compare_models_across_datasets(all_results, metric='accuracy', save_path='comparison_accuracy.png')
