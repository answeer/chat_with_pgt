import pandas as pd
import numpy as np
import re
import os
from itertools import zip_longest
import matplotlib.pyplot as plt
from collections import defaultdict
from editdistance import eval as edit_distance
import time
from tqdm import tqdm

def filter_empty_text(df, is_gt=False):
    """过滤空文本并验证必要字段"""
    df = df.copy()
    required_gt_cols = ['line_no', 'text', 'x_min', 'y_min', 'x_max', 'y_max']
    required_ocr_cols = ['text', 'x_min', 'y_min', 'x_max', 'y_max']
    
    if is_gt:
        missing = [col for col in required_gt_cols if col not in df.columns]
    else:
        missing = [col for col in required_ocr_cols if col not in df.columns]
    
    if missing:
        raise ValueError(f"Missing required columns: {', '.join(missing)}")
    
    initial_count = len(df)
    df = df[df['text'].notna() & (df['text'].str.strip() != '')]
    filtered_count = initial_count - len(df)
    
    if filtered_count > 0:
        print(f"Filtered {filtered_count} empty text entries")
    
    return df

def calculate_centers(df):
    """计算bounding box的中心点"""
    df = df.copy()
    for col in ['x_min', 'y_min', 'x_max', 'y_max']:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    df['x_center'] = (df['x_min'] + df['x_max']) / 2
    df['y_center'] = (df['y_min'] + df['y_max']) / 2
    return df

def adaptive_line_grouping(df, tolerance_factor=0.5, min_line_height=5):
    """改进的行分组算法"""
    if len(df) == 0:
        return []
    
    df = df.copy()
    df['height'] = df['y_max'] - df['y_min']
    df = df[df['height'] >= min_line_height]
    
    if len(df) == 0:
        return []
    
    avg_height = df['height'].mean()
    median_height = df['height'].median()
    ref_height = (avg_height * 0.3 + median_height * 0.7)
    tolerance = ref_height * tolerance_factor
    
    sorted_df = df.sort_values(by='y_center')
    lines = []
    current_line = []
    last_y_center = None
    last_max_y = -np.inf
    
    for _, row in sorted_df.iterrows():
        current_y = row['y_center']
        
        if last_y_center is None:
            current_line.append(row)
            last_y_center = current_y
            last_max_y = row['y_max']
            continue
        
        has_overlap = row['y_min'] < last_max_y
        y_diff = abs(current_y - last_y_center)
        can_merge = y_diff <= tolerance or has_overlap
        
        if can_merge:
            current_line.append(row)
            heights = [r['height'] for r in current_line]
            y_centers = [r['y_center'] for r in current_line]
            last_y_center = np.average(y_centers, weights=heights)
            last_max_y = max(last_max_y, row['y_max'])
        else:
            lines.append(current_line)
            current_line = [row]
            last_y_center = current_y
            last_max_y = row['y_max']
    
    if current_line:
        lines.append(current_line)
    
    return lines

def is_chinese(char):
    """检查字符是否为中文"""
    return '\u4e00' <= char <= '\u9fff' or char in "，。、；：？！" "“”‘’（）【】…—《》"

def smart_join(texts):
    """改进的中英文混合文本智能连接"""
    if not texts:
        return ""
    
    result = texts[0].strip()
    
    for i in range(1, len(texts)):
        prev_text = texts[i-1].strip()
        curr_text = texts[i].strip()
        
        if not prev_text or not curr_text:
            result += curr_text
            continue
        
        prev_last = prev_text[-1] if prev_text else ''
        curr_first = curr_text[0] if curr_text else ''
        
        add_space = False
        
        if (prev_last.isalpha() or prev_last.isdigit()) and \
           (curr_first.isalpha() or curr_first.isdigit()):
            add_space = True
        elif is_chinese(prev_last) and curr_first.isalpha():
            add_space = False
        elif prev_last.isalpha() and is_chinese(curr_first):
            add_space = False
        elif prev_last in ',.!?;:' and curr_first.isalpha():
            add_space = True
        elif prev_last in ',.!?;:' and is_chinese(curr_first):
            add_space = False
        
        result += " " + curr_text if add_space else curr_text
    
    return result

def process_gt(gt_df):
    """处理Ground Truth数据"""
    gt_df = filter_empty_text(gt_df, is_gt=True)
    
    # 确保包含所有必需的列
    if 'line_no' not in gt_df.columns:
        gt_df['line_no'] = np.arange(len(gt_df))
    
    gt_df['line_no'] = gt_df['line_no'].astype(int)
    gt_df = calculate_centers(gt_df)
    
    # 按行号分组
    grouped = gt_df.groupby('line_no')
    merged_lines = []
    
    for line_no, group in sorted(grouped, key=lambda x: x[0]):
        sorted_group = group.sort_values(by='x_center')
        merged_lines.append("".join(sorted_group['text'].tolist()))
    
    return "".join(merged_lines)

def process_ocr(ocr_df, tolerance_factor=0.5, min_line_height=5):
    """处理OCR结果数据"""
    ocr_df = filter_empty_text(ocr_df)
    ocr_df = calculate_centers(ocr_df)
    
    # 自适应行分组
    lines = adaptive_line_grouping(
        ocr_df, 
        tolerance_factor=tolerance_factor,
        min_line_height=min_line_height
    )
    
    line_texts = []
    for line in lines:
        sorted_line = sorted(line, key=lambda r: r['x_center'])
        line_text = "".join(r['text'] for r in sorted_line)
        line_texts.append(line_text)
    
    return "".join(line_texts)

def word_accuracy(gt_text, ocr_text):
    """计算单词准确率（混合中英文）"""
    # 中文分词：每个中文字符作为独立词
    chinese_chars = []
    for char in gt_text:
        if is_chinese(char):
            chinese_chars.append(char)
    
    def tokenize(text):
        tokens = []
        current = ""
        for char in text:
            if is_chinese(char):
                if current:
                    tokens.append(current)
                    current = ""
                tokens.append(char)
            elif char.isspace() or char in r"""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~""":
                if current:
                    tokens.append(current)
                    current = ""
                tokens.append(char)
            else:
                current += char
        if current:
            tokens.append(current)
        return tokens
    
    gt_tokens = tokenize(gt_text)
    ocr_tokens = tokenize(ocr_text)
    
    # 计算最小编辑距离
    m, n = len(gt_tokens), len(ocr_tokens)
    dp = [[0] * (n+1) for _ in range(m+1)]
    
    for i in range(m+1):
        for j in range(n+1):
            if i == 0:
                dp[i][j] = j
            elif j == 0:
                dp[i][j] = i
            else:
                cost = 0 if gt_tokens[i-1] == ocr_tokens[j-1] else 1
                dp[i][j] = min(
                    dp[i-1][j] + 1,
                    dp[i][j-1] + 1,
                    dp[i-1][j-1] + cost
                )
    
    word_edit_dist = dp[m][n]
    max_tokens = max(m, n)
    
    if max_tokens == 0:
        return 1.0
    return 1 - (word_edit_dist / max_tokens)

def evaluate_single_image(gt_path, ocr_path, verbose=False, 
                          tolerance_factor=0.5, min_line_height=5):
    """
    评估单个图片的OCR精度
    返回包含评估结果的字典
    """
    try:
        gt_df = pd.read_csv(gt_path)
        ocr_df = pd.read_excel(ocr_path)
        
        if verbose:
            print(f"Processing: {os.path.basename(gt_path)}")
            print(f"  Ground Truth entries: {len(gt_df)}")
            print(f"  OCR entries: {len(ocr_df)}")
        
        # 处理文本
        gt_text = process_gt(gt_df)
        ocr_text = process_ocr(ocr_df, 
                              tolerance_factor=tolerance_factor,
                              min_line_height=min_line_height)
        
        # 计算准确率
        char_acc = 1 - edit_distance(gt_text, ocr_text)/max(len(gt_text), len(ocr_text), 1)
        word_acc = word_accuracy(gt_text, ocr_text)
        
        return {
            "image_name": os.path.basename(gt_path),
            "char_accuracy": char_acc,
            "word_accuracy": word_acc,
            "char_accuracy_str": f"{char_acc:.4%}",
            "word_accuracy_str": f"{word_acc:.4%}",
            "gt_length": len(gt_text),
            "ocr_length": len(ocr_text),
            "status": "success",
            "error": None
        }
    
    except Exception as e:
        import traceback
        error_msg = f"Error processing {gt_path}: {str(e)}"
        if verbose:
            print(error_msg)
            traceback.print_exc()
        
        return {
            "image_name": os.path.basename(gt_path),
            "char_accuracy": 0.0,
            "word_accuracy": 0.0,
            "char_accuracy_str": "0.00%",
            "word_accuracy_str": "0.00%",
            "gt_length": 0,
            "ocr_length": 0,
            "status": "error",
            "error": error_msg
        }

def batch_evaluate(gt_dir, ocr_base_dir, 
                  output_csv="ocr_evaluation_results.csv",
                  tolerance_factor=0.5, min_line_height=5,
                  verbose=False):
    """
    批量评估数据集中的所有图片
    
    参数:
        gt_dir: 包含所有Ground Truth CSV文件的目录
        ocr_base_dir: OCR结果的基础目录，包含各个图片命名的子文件夹
        output_csv: 评估结果输出文件
        tolerance_factor: 行分组容差系数
        min_line_height: 最小行高阈值
        verbose: 是否显示详细处理信息
    """
    # 创建结果容器
    results = []
    start_time = time.time()
    
    # 获取所有GT文件
    gt_files = [f for f in os.listdir(gt_dir) if f.endswith('.csv')]
    image_names = [os.path.splitext(f)[0] for f in gt_files]
    
    if verbose:
        print(f"Found {len(gt_files)} ground truth files")
        print(f"Starting evaluation of {len(gt_files)} images...")
    
    # 进度条
    pbar = tqdm(total=len(gt_files), desc="Evaluating images")
    
    # 遍历所有图片
    for gt_file in gt_files:
        # 获取图片名
        image_name = os.path.splitext(gt_file)[0]
        
        # 构建文件路径
        gt_path = os.path.join(gt_dir, gt_file)
        ocr_dir = os.path.join(ocr_base_dir, image_name, "TextExtraction")
        
        # 查找OCR结果文件
        ocr_file = None
        for f in os.listdir(ocr_dir):
            if f.endswith('.xlsx') and image_name in f:
                ocr_file = f
                break
        
        if not ocr_file:
            if verbose:
                print(f"OCR file not found for {image_name} in {ocr_dir}")
            result = {
                "image_name": image_name,
                "char_accuracy": 0.0,
                "word_accuracy": 0.0,
                "char_accuracy_str": "0.00%",
                "word_accuracy_str": "0.00%",
                "gt_length": 0,
                "ocr_length": 0,
                "status": "error",
                "error": f"OCR file not found in {ocr_dir}"
            }
            results.append(result)
            pbar.update(1)
            continue
        
        ocr_path = os.path.join(ocr_dir, ocr_file)
        
        # 评估单个图片
        result = evaluate_single_image(
            gt_path, ocr_path, 
            verbose=verbose,
            tolerance_factor=tolerance_factor,
            min_line_height=min_line_height
        )
        
        results.append(result)
        pbar.update(1)
    
    pbar.close()
    
    # 转换为DataFrame
    results_df = pd.DataFrame(results)
    
    # 计算平均值
    if not results_df.empty:
        avg_char_acc = results_df['char_accuracy'].mean()
        avg_word_acc = results_df['word_accuracy'].mean()
        
        avg_results = {
            "image_name": "AVERAGE",
            "char_accuracy": avg_char_acc,
            "word_accuracy": avg_word_acc,
            "char_accuracy_str": f"{avg_char_acc:.4%}",
            "word_accuracy_str": f"{avg_word_acc:.4%}",
            "gt_length": results_df['gt_length'].sum(),
            "ocr_length": results_df['ocr_length'].sum(),
            "status": "success",
            "error": None
        }
        
        # 创建摘要行
        summary_df = pd.DataFrame([avg_results])
        results_df = pd.concat([results_df, summary_df], ignore_index=True)
    
    # 保存结果
    results_df.to_csv(output_csv, index=False)
    
    total_time = time.time() - start_time
    print(f"\nEvaluation completed in {total_time:.2f} seconds")
    print(f"Results saved to: {output_csv}")
    
    # 打印摘要统计
    if not results_df.empty:
        success_count = results_df[results_df['status'] == 'success'].shape[0] - 1  # 减去平均值行
        error_count = results_df[results_df['status'] == 'error'].shape[0]
        
        print("\n===== Evaluation Summary =====")
        print(f"Total images evaluated: {len(gt_files)}")
        print(f"Successfully evaluated: {success_count}")
        print(f"Failed evaluations: {error_count}")
        
        if success_count > 0:
            print(f"\nAverage Character Accuracy: {avg_results['char_accuracy_str']}")
            print(f"Average Word Accuracy: {avg_results['word_accuracy_str']}")
            print(f"Total GT characters: {avg_results['gt_length']}")
            print(f"Total OCR characters: {avg_results['ocr_length']}")
    
    return results_df

def visualize_results(results_df, output_dir="evaluation_plots"):
    """
    可视化评估结果
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # 过滤掉平均值行
    plot_df = results_df[results_df['image_name'] != 'AVERAGE']
    
    if plot_df.empty:
        print("No data to visualize")
        return
    
    # 创建图表
    plt.figure(figsize=(15, 10))
    
    # 字符准确率分布
    plt.subplot(2, 2, 1)
    plt.hist(plot_df['char_accuracy'], bins=20, alpha=0.7)
    plt.title('Character Accuracy Distribution')
    plt.xlabel('Accuracy')
    plt.ylabel('Number of Images')
    
    # 单词准确率分布
    plt.subplot(2, 2, 2)
    plt.hist(plot_df['word_accuracy'], bins=20, alpha=0.7, color='green')
    plt.title('Word Accuracy Distribution')
    plt.xlabel('Accuracy')
    plt.ylabel('Number of Images')
    
    # 准确率箱线图
    plt.subplot(2, 2, 3)
    data = [plot_df['char_accuracy'], plot_df['word_accuracy']]
    plt.boxplot(data, labels=['Char Accuracy', 'Word Accuracy'])
    plt.title('Accuracy Boxplot')
    plt.ylabel('Accuracy')
    
    # 准确率散点图
    plt.subplot(2, 2, 4)
    plt.scatter(plot_df['char_accuracy'], plot_df['word_accuracy'], alpha=0.7)
    plt.plot([0, 1], [0, 1], 'r--')
    plt.title('Character vs Word Accuracy')
    plt.xlabel('Character Accuracy')
    plt.ylabel('Word Accuracy')
    plt.grid(True)
    
    plt.tight_layout()
    plot_path = os.path.join(output_dir, "accuracy_plots.png")
    plt.savefig(plot_path)
    print(f"Visualizations saved to: {plot_path}")

def analyze_errors(results_df, output_csv="error_analysis.csv"):
    """
    分析评估错误
    """
    # 过滤出有错误的图片
    error_df = results_df[results_df['status'] == 'error']
    
    if error_df.empty:
        print("No errors found in evaluation")
        return
    
    # 分析错误类型
    error_df['error_type'] = error_df['error'].apply(
        lambda x: "File not found" if "not found" in str(x) else 
                  "Data processing" if "columns" in str(x) or "value" in str(x) else 
                  "OCR conversion" if "format" in str(x) or "read" in str(x) else 
                  "Other"
    )
    
    # 保存错误分析
    error_df.to_csv(output_csv, index=False)
    print(f"Error analysis saved to: {output_csv}")
    
    # 打印错误统计
    error_counts = error_df['error_type'].value_counts()
    print("\n===== Error Analysis Summary =====")
    print(f"Total errors: {len(error_df)}")
    print("Error type distribution:")
    print(error_counts)

if __name__ == "__main__":
    # 配置路径
    GT_DIR = "path/to/ground_truth_directory"  # 包含所有CSV文件的目录
    OCR_BASE_DIR = "path/to/ocr_results_base_directory"  # 包含图片命名子文件夹的根目录
    
    # 批量评估
    results_df = batch_evaluate(
        gt_dir=GT_DIR,
        ocr_base_dir=OCR_BASE_DIR,
        output_csv="ocr_evaluation_results.csv",
        tolerance_factor=0.3,
        min_line_height=5,
        verbose=True  # 设置为False以减少输出
    )
    
    # 结果可视化
    visualize_results(results_df)
    
    # 错误分析
    analyze_errors(results_df)
