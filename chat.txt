\textbf{Language Modeling Loss ($L_{mlm}$)}:

\[ L_{mlm} = -\sum_{i} \log P_{\text{student}}(w_i | w_{\text{context}}) \]

where \(w_i\) is the masked word, \(w_{\text{context}}\) is its context, and \(P_{\text{student}}\) is the prediction probability by the student model.

\textbf{Distillation Loss ($L_{ce}$)}:

\[ L_{ce} = -\sum_{i} t_i \log(s_i) \]

where \(t_i\) is the soft target probability from the teacher model, and \(s_i\) is the prediction probability by the student model.

To smooth the output distribution, the softened softmax is used:

\[ p_i = \frac{\exp(z_i / T)}{\sum_{j} \exp(z_j / T)} \]

where \(T\) is the temperature parameter and \(z_i\) are the logits from the model.

\textbf{Cosine-Distance Loss ($L_{cos}$)}:

\[ L_{cos} = 1 - \cos(h_{\text{teacher}}, h_{\text{student}}) \]

where \(h_{\text{teacher}}\) and \(h_{\text{student}}\) are the hidden state vectors from the teacher and student models, respectively, and \(\cos\) represents the cosine similarity between the two vectors.

The final training objective is a linear combination of these three loss functions:

\[ L = \alpha L_{mlm} + \beta L_{ce} + \gamma L_{cos} \]

where \(\alpha\), \(\beta\), and \(\gamma\) are weight coefficients used to balance the contributions of the three loss functions in the total loss.
