import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import time
import json
from pathlib import Path
from PIL import Image
import os

# Configuration
model_path = r"C:\Users\1657820\Documents\models\nuext"
directory = r"C:\Users\1657820\Documents\datasets\UAT_Latest"
batch_size = 4  # Adjust based on GPU memory
template = """{
    'date': '',
    'shipper name': '',
    'shipper address': '',
    'shipper country': '',
    'transport document number': '',
    'plase of issue': '',
    'notify party name': '',
    'notify party address': '',
    'vessel': '',
    'agent name': '',
    'agent address': '',
    'agent country': ''
}"""

# Load model and processor
model = AutoModelForVision2Seq.from_pretrained(
    model_path,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side='left'
)

# Prepare results container
results = {
    "file_name": [],
    "inference_results": [],
    "inference_time": []
}

# Collect image paths
image_paths = []
valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
for path in Path(directory).iterdir():
    if path.suffix.lower() in valid_extensions:
        image_paths.append(path)

# Process in batches
for i in range(0, len(image_paths), batch_size):
    batch_paths = image_paths[i:i+batch_size]
    batch_images = []
    batch_texts = []
    
    # Prepare batch
    for path in batch_paths:
        # Load and convert image
        img = Image.open(path).convert('RGB')
        batch_images.append(img)
        
        # Create message
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": path},
                ],
            }
        ]
        
        # Apply template
        text = processor.tokenizer.apply_chat_template(
            messages,
            template=template,
            tokenize=False,
            add_generation_prompt=True
        )
        batch_texts.append(text)

    # Process batch
    inputs = processor(
        text=batch_texts,
        images=batch_images,
        padding=True,
        return_tensors="pt",
    ).to("cuda")

    # Inference
    start_time = time.time()
    generation_config = {"do_sample": False, "num_beams": 1, "max_new_tokens": 2048}
    generated_ids = model.generate(**inputs, **generation_config)
    batch_time = time.time() - start_time()
    
    # Decode results
    generated_ids_trimmed = [
        out_ids[len(in_ids):] 
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_texts = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )
    
    # Store results
    time_per_image = batch_time / len(batch_paths)
    for path, output in zip(batch_paths, output_texts):
        results["file_name"].append(path.name)
        results["inference_results"].append(output)
        results["inference_time"].append(time_per_image)
    
    print(f"Processed batch {i//batch_size + 1}/{(len(image_paths)-1)//batch_size + 1}")

# Save results
with open('nuext_results.json', 'w') as f:
    json.dump(results, f, indent=4)

print("Processing complete. Results saved to nuext_results.json")
