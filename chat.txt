from llm_sanitation.scanners.scanner_base import Scanner
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from llm_sanitation.utils.common import read_config
from llm_sanitation.logging.logging_setup import LogUtil, LogType, LogLevel


class RemoveStopwords(Scanner):

    def __init__(self, **kwargs):
        sanitize = kwargs["sanitize"]
        super().__init__("remove_stopwords", 0.5,sanitize=sanitize)
        self.stop_words = set(stopwords.words('english'))
        config = read_config("harmful_strings.yml")
        # custom_stopwords = config.get("custom_stopwords", [])
        # self.stop_words = set(list(self.stop_words).extend(custom_stopwords))

    def predict(self, data):
        sanitized_data = data
        score = 0
        try:
            tokens = word_tokenize(data)
            result = [i for i in tokens if not i in self.stop_words]
            sanitized_data =  " ".join(result)
            score = 1
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, e)

        return score, sanitized_data

    def format_response(self):
        self.response["score"] = self.pred[0]
        self.response['sanitized_data'] = self.pred[1]
