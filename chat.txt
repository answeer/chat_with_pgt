The prepare_item_data function is a utility designed to preprocess and normalize input data (text, HTML, etc.) for machine learning tasks (both training and inference). Here's a breakdown of what it does:
Key Steps:

Data Truncation:
Checks if the input data exceeds 102,400 bytes (100KB). If so, truncates it to the first 100KB to avoid processing overly large inputs.
Keyword Handling:
Extracts values from the keywords dictionary for keys specified in prepend_keywords.
Appends the first value of each keyword (if it exists) to the cleaned content. These keywords are cleaned in "email" mode (replaces @ and . with underscores).
Title Cleaning:
Cleans the title using clean_text with the specified norm_mode (e.g., removing special characters, normalizing whitespace).
HTML/Data Cleaning:
Cleans the data (assumed to be HTML) using clean_html, which:
Removes unwanted HTML tags (e.g., <script>, <style>).
Applies text normalization based on norm_mode.
Combining Parts:
Joins the cleaned keyword values, title, and data into a single normalized string (clean_data).
Cleaning Modes (norm_mode):

The function uses different text normalization strategies:
"alpha": Keeps only alphabetic characters and spaces, removes URLs/emails, converts to lowercase.
"alphanum": Keeps alphanumeric characters, removes stopwords, normalizes whitespace.
"unicode": Handles Unicode characters, replaces special symbols with underscores, splits compound words.
"email": Replaces @ and . with underscores (used for keyword cleaning).
Purpose:

Standardization: Ensures input data (text/HTML) is in a consistent format for machine learning models.
Noise Reduction: Removes irrelevant content (HTML tags, special characters, stopwords).
Size Control: Truncates large inputs to avoid computational overhead.
Example Use Case:

If you have a webpage (HTML data) with a title and keywords, this function:
Truncates the HTML if too large.
Extracts and cleans keywords.
Cleans the title and HTML content.
Combines everything into a normalized string for the model.
This is useful for tasks like text classification, search relevance, or NLP pipelines.
