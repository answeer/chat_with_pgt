import pandas as pd
import numpy as np
import re
from itertools import zip_longest
import matplotlib.pyplot as plt
from collections import defaultdict
from editdistance import eval as edit_distance

def filter_empty_text(df, is_gt=False):
    """过滤空文本并验证必要字段"""
    # 复制数据避免修改原始DataFrame
    df = df.copy()
    
    # 验证必要字段存在
    required_gt_cols = ['line_no', 'text', 'x_min', 'y_min', 'x_max', 'y_max']
    required_ocr_cols = ['text', 'x_min', 'y_min', 'x_max', 'y_max']
    
    if is_gt:
        missing = [col for col in required_gt_cols if col not in df.columns]
    else:
        missing = [col for col in required_ocr_cols if col not in df.columns]
    
    if missing:
        raise ValueError(f"缺少必要列: {', '.join(missing)}")
    
    # 过滤空文本
    initial_count = len(df)
    df = df[df['text'].notna() & (df['text'].str.strip() != '')]
    filtered_count = initial_count - len(df)
    
    if filtered_count > 0:
        print(f"已过滤 {filtered_count} 个空文本条目")
    
    return df

def calculate_centers(df):
    """计算bounding box的中心点"""
    df = df.copy()
    # 确保坐标是数值类型
    for col in ['x_min', 'y_min', 'x_max', 'y_max']:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # 计算中心点
    df['x_center'] = (df['x_min'] + df['x_max']) / 2
    df['y_center'] = (df['y_min'] + df['y_max']) / 2
    return df

def adaptive_line_grouping(df, tolerance_factor=0.5, min_line_height=5):
    """
    改进的行分组算法
    参数:
        tolerance_factor: 容差系数 (0.4-0.6)
        min_line_height: 最小行高阈值 (避免微小框单独成行)
    """
    if len(df) == 0:
        return []
    
    # 计算每个框的高度
    df = df.copy()
    df['height'] = df['y_max'] - df['y_min']
    
    # 过滤高度过小的框（可能是噪声）
    df = df[df['height'] >= min_line_height]
    
    if len(df) == 0:
        return []
    
    # 计算平均行高
    avg_height = df['height'].mean()
    median_height = df['height'].median()
    
    # 使用中位数和平均值的加权值（更鲁棒）
    ref_height = (avg_height * 0.3 + median_height * 0.7)
    
    # 容差阈值 = 参考高度 * 容差系数
    tolerance = ref_height * tolerance_factor
    
    # 按y中心点排序
    sorted_df = df.sort_values(by='y_center')
    
    # 改进的行分组
    lines = []
    current_line = []
    last_y_center = None
    
    for _, row in sorted_df.iterrows():
        current_y = row['y_center']
        
        # 首次处理
        if last_y_center is None:
            current_line.append(row)
            last_y_center = current_y
            continue
        
        # 计算与当前行中心的差异
        y_diff = abs(current_y - last_y_center)
        
        # 判断是否属于当前行
        if y_diff <= tolerance:
            current_line.append(row)
            
            # 更新行中心点（高度加权平均）
            heights = [r['height'] for r in current_line]
            y_centers = [r['y_center'] for r in current_line]
            last_y_center = np.average(y_centers, weights=heights)
        
        # 判断是否是相邻行（可能是同一段落）
        elif y_diff <= tolerance * 2.5:
            # 检查是否应该合并（如小写字母延伸部分）
            current_line_max_y = max(r['y_max'] for r in current_line) if current_line else 0
            next_line_min_y = row['y_min']
            
            # 如果当前行底部与下一行顶部有重叠
            if next_line_min_y <= current_line_max_y + tolerance:
                current_line.append(row)
                # 更新行中心点
                heights = [r['height'] for r in current_line]
                y_centers = [r['y_center'] for r in current_line]
                last_y_center = np.average(y_centers, weights=heights)
            else:
                # 新行开始
                lines.append(current_line)
                current_line = [row]
                last_y_center = current_y
        else:
            # 明显的新行
            lines.append(current_line)
            current_line = [row]
            last_y_center = current_y
    
    # 添加最后一行
    if current_line:
        lines.append(current_line)
    
    return lines

def is_chinese(char):
    """检查字符是否为中文"""
    return '\u4e00' <= char <= '\u9fff'

def smart_join(texts):
    """
    智能连接文本块，减少多余空格
    处理以下情况：
     1. 标点符号后不应有空格
     2. 中文文本不应有空格
     3. 连续数字/字母应保持连接
    """
    if not texts:
        return ""
    
    # 特殊处理：如果整个文本是中文，直接连接
    all_chinese = all(is_chinese(char) for text in texts for char in text)
    if all_chinese:
        return "".join(texts)
    
    result = texts[0].strip()
    
    for i in range(1, len(texts)):
        prev_text = texts[i-1].strip()
        curr_text = texts[i].strip()
        
        if not prev_text or not curr_text:
            result += " " + curr_text
            continue
        
        prev_last = prev_text[-1]
        curr_first = curr_text[0]
        
        # 检查是否需要添加空格
        add_space = True
        
        # 前一个以标点结束
        if prev_last in ',.;:?!。，、；：？！）】”':
            add_space = False
        
        # 当前以标点开始
        elif curr_first in '([{（【“':
            add_space = False
        
        # 中文文本（前后都是中文字符）
        elif is_chinese(prev_last) and is_chinese(curr_first):
            add_space = False
        
        # 数字/字母连续
        elif prev_last.isalnum() and curr_first.isalnum():
            add_space = True
        
        # 特殊符号处理（如$、#等）
        elif not prev_last.isalnum() and not curr_first.isalnum():
            add_space = False
        
        result += " " + curr_text if add_space else curr_text
    
    return result

def process_gt(gt_df):
    """处理Ground Truth数据"""
    # 过滤空文本
    gt_df = filter_empty_text(gt_df, is_gt=True)
    
    # 确保line_no是整数
    gt_df['line_no'] = gt_df['line_no'].astype(int)
    
    # 计算中心点
    gt_df = calculate_centers(gt_df)
    
    # 按行号分组
    grouped = gt_df.groupby('line_no')
    
    merged_lines = []
    for line_no, group in sorted(grouped, key=lambda x: x[0]):
        # 按x中心点排序并拼接文本
        sorted_group = group.sort_values(by='x_center')
        line_text = " ".join(sorted_group['text'].tolist())
        merged_lines.append(line_text)
    
    return smart_join(merged_lines)

def process_ocr(ocr_df, tolerance_factor=0.5, min_line_height=5):
    """处理OCR结果数据 - 移除所有换行符"""
    # 过滤空文本
    ocr_df = filter_empty_text(ocr_df)
    
    # 计算中心点
    ocr_df = calculate_centers(ocr_df)
    
    # 自适应行分组
    lines = adaptive_line_grouping(
        ocr_df, 
        tolerance_factor=tolerance_factor,
        min_line_height=min_line_height
    )
    
    # 提取所有文本块（按从上到下、从左到右顺序）
    all_texts = []
    for line in lines:
        # 按x中心点排序（行内从左到右）
        sorted_line = sorted(line, key=lambda r: r['x_center'])
        all_texts.extend([r['text'] for r in sorted_line])
    
    # 智能连接所有文本块
    return smart_join(all_texts)

def evaluate_ocr(gt_path, ocr_path, 
                tolerance_factor=0.5, 
                min_line_height=5):
    """主评估函数 - 移除所有换行符"""
    try:
        # 读取数据
        gt_df = pd.read_csv(gt_path)
        ocr_df = pd.read_excel(ocr_path)
        
        print(f"Ground Truth 原始条目数: {len(gt_df)}")
        print(f"OCR 结果原始条目数: {len(ocr_df)}")
        
        # 处理文本（移除所有换行符）
        gt_text = process_gt(gt_df)
        ocr_text = process_ocr(ocr_df, 
                              tolerance_factor=tolerance_factor,
                              min_line_height=min_line_height)
        
        print("\n处理后的Ground Truth文本:")
        print(gt_text[:1000] + "..." if len(gt_text) > 1000 else gt_text)
        
        print("\n处理后的OCR文本:")
        print(ocr_text[:1000] + "..." if len(ocr_text) > 1000 else ocr_text)
        
        # 计算准确率
        # char_acc = char_accuracy(gt_text, ocr_text)
        char_acc = 1 - edit_distance(gt_text, ocr_text)/max(len(gt_text), len(ocr_text))
        word_acc = word_accuracy(gt_text,ocr_text)
        
        return {
            "char_accuracy": f"{char_acc:.4%}",
            "word_accuracy": f"{word_acc:.4%}",
            "gt_text": gt_text,
            "ocr_text": ocr_text,
            "gt_length": len(gt_text),
            "ocr_length": len(ocr_text)
        }
    
    except Exception as e:
        import traceback
        print(f"处理过程中发生错误: {str(e)}")
        traceback.print_exc()
        return {
            "error": str(e),
            "char_accuracy": "0.00%",
            "word_accuracy": "0.00%"
        }

if __name__ == "__main__":

    results = evaluate_ocr(
        "82092117.csv", 
        r"C:\Users\1657820\Desktop\51433-swoosh-textxtract-v4\funsd_add_noise_denoise_docres\82092117.jpg\TextExtraction\82092117_end2end.jpg_page1_extracted_data.xlsx",
        tolerance_factor=0.3,
        min_line_height=5
    )
    
    if "error" not in results:
        print("\n===== 评估结果 =====")
        print(f"字符准确率: {results['char_accuracy']}")
        print(f"单词准确率: {results['word_accuracy']}")
        print(f"\n文本长度统计:")
        print(f"Ground Truth 总字符数: {results['gt_length']}")
        print(f"OCR 结果总字符数: {results['ocr_length']}")
