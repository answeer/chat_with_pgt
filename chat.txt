from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
import random
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from minicheck.minicheck import MiniCheck
from llm_guard.output_scanners import FactualConsistency
import tqdm
import time

class EvalDataset:
    def __init__(self, data_path='augmented_dataset.xlsx'):
        self.data_path = data_path
        self.tokenizer = AutoTokenizer.from_pretrained('roberta-large')  # 用于计算token数

    def load_dataset(self):
        # 加载增广后的数据集
        df = pd.read_excel(self.data_path)

        # 确保 'Context' 和 'Answer' 为字符串类型
        df['Context'] = df['Context'].apply(lambda x: str(x) if not isinstance(x, str) else x)
        df['Answer'] = df['Answer'].apply(lambda x: str(x) if not isinstance(x, str) else x)
        return df

    def count_tokens(self, context, response):
        inputs = self.tokenizer(context, response, return_tensors='pt', truncation=True)
        return len(inputs['input_ids'][0])  # 返回token数

    def run(self, model_nm):
        df = self.load_dataset()
        context_list = df['Context'].tolist()
        response_list = df['Answer'].tolist()
        true_labels = df['Label'].tolist()  # 真实标签

        # 初始化列表以记录每个样本的推理时间、token 数和预测标签
        inference_times = []
        token_counts = []
        predictions = []

        # 使用 tqdm 显示进度
        for context, response in tqdm.tqdm(zip(context_list, response_list), total=len(context_list)):
            tokens = self.count_tokens(context, response)
            token_counts.append(tokens)

            try:
                start_inference = time.time()
                if model_nm == "minicheck":
                    scorer = MiniCheck(model_name='roberta-large', cache_dir='./ckpts')
                    pred_label, _, _, _ = scorer.score(docs=[context], claims=[response])
                    pred_label = int(pred_label[0])  # 获取模型预测的标签（假设输出为0或1）
                
                elif model_nm == "hhem":
                    model = AutoModelForSequenceClassification.from_pretrained('models/hallucination_evaluation_model', trust_remote_code=True)
                    tokenizer = AutoTokenizer.from_pretrained('models/hallucination_evaluation_model', trust_remote_code=True)
                    inputs = tokenizer(context, response, return_tensors='pt', truncation=True)

                    # 计算模型输出
                    with torch.no_grad():
                        outputs = model(**inputs)
                        logits = outputs.logits

                    # 使用 sigmoid 激活并转换为概率
                    probabilities = torch.sigmoid(logits)
                    pred_label = (probabilities >= 0.5).int().item()  # 1 if score >= 0.5, else 0
                
                elif model_nm == 'factual_consistency':
                    model = FactualConsistency(minimum_score=0.5)
                    _, is_valid, _ = model.scan(context, response)
                    pred_label = int(is_valid)

                end_inference = time.time()

                # 记录推理时间和预测标签
                inference_times.append(end_inference - start_inference)
                predictions.append(pred_label)

            except Exception as e:
                print(f"Error occurred for model {model_nm}: {e}")
                inference_times.append(0)
                predictions.append(None)

        # 计算并输出评估指标
        self.calculate_metrics(true_labels, predictions, model_nm)

        return inference_times, token_counts, predictions

    def calculate_metrics(self, true_labels, predictions, model_nm):
        # 过滤掉出错导致的 None 值
        valid_indices = [i for i, pred in enumerate(predictions) if pred is not None]
        true_labels = [true_labels[i] for i in valid_indices]
        predictions = [predictions[i] for i in valid_indices]

        # 输出分类报告
        print(f"\nEvaluation metrics for {model_nm}:")
        print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))

    def plot_results(self, results, save_path='model_inference_plot.png'):
        # 绘制 token 数和推理时间的关系图
        pass

    def save_results_to_excel(self, df, results, file_name='model_inference_results.xlsx'):
        # 保存推理结果到 Excel 文件
        pass


if __name__ == "__main__":
    model_list = ['minicheck', 'hhem', 'factual_consistency']
    eval_dataset = EvalDataset()

    # 存放每个模型的推理时间和token数的结果
    results = {}

    # 加载数据集
    df = eval_dataset.load_dataset()

    # 逐个模型运行并记录结果
    for model_nm in model_list:
        print(f"Running evaluation for {model_nm}...")
        inference_times, token_counts, predictions = eval_dataset.run(model_nm)
        results[model_nm] = (inference_times, token_counts, predictions)
