import io
import numpy as np
import cv2
from PIL import Image
from flask import Flask, request, jsonify
from flask_cors import CORS
from paddleocr import PaddleOCR

app = Flask(__name__)
CORS(app)

# 轻量化配置：中文/英文常见场景
ocr = PaddleOCR(use_angle_cls=True, lang='ch', det_db_score_mode='fast')

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = "".join(ch for ch in s.strip())
    s = s.replace("\u3000"," ").replace("\xa0"," ")
    return s.lower()

def lines_to_blocks(lines, y_gap=10, x_shift=80):
    """
    将相邻且排版接近的行聚合为“块”(block)，便于匹配多行实体（如长地址）。
    y_gap: 行中心 y 距离阈值（像素）
    x_shift: 左边缘允许的偏移（像素）
    """
    if not lines:
        return []

    def line_key(l):
        poly = np.array(l["polygon"], dtype=np.float32)
        x = float(np.min(poly[:,0]))
        y = float(np.min(poly[:,1]))
        return (y, x)

    lines_sorted = sorted(lines, key=line_key)
    blocks = []
    current = [lines_sorted[0]]

    def line_feat(l):
        poly = np.array(l["polygon"], dtype=np.float32)
        x_left = float(np.min(poly[:,0]))
        y_top  = float(np.min(poly[:,1]))
        y_center = float(np.mean(poly[:,1]))
        return x_left, y_top, y_center

    prev_x, prev_y, prev_yc = line_feat(lines_sorted[0])

    for l in lines_sorted[1:]:
        x, y, yc = line_feat(l)
        if abs(yc - prev_yc) <= y_gap and abs(x - prev_x) <= x_shift:
            current.append(l)
        else:
            blocks.append(current)
            current = [l]
        prev_x, prev_y, prev_yc = x, y, yc

    if current:
        blocks.append(current)

    block_objs = []
    for bid, group in enumerate(blocks):
        group_sorted = sorted(group, key=lambda l: np.min(np.array(l["polygon"])[:,0]))
        text = " ".join([l["text"] for l in group_sorted])
        norm_text = normalize_text(text)
        all_pts = np.concatenate([np.array(l["polygon"]) for l in group_sorted], axis=0)
        x1, y1 = np.min(all_pts[:,0]), np.min(all_pts[:,1])
        x2, y2 = np.max(all_pts[:,0]), np.max(all_pts[:,1])
        block_objs.append({
            "block_id": bid,
            "text": text,
            "norm_text": norm_text,
            "lines": group_sorted,
            "display_box": [float(x1), float(y1), float(x2), float(y2)],
        })
    return block_objs

def ocr_image(pil_img):
    img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    result = ocr.ocr(img, cls=True)
    lines = []
    for idx in range(len(result)):
        res = result[idx]
        for line in res:
            poly, (txt, conf) = line
            lines.append({
                "text": txt,
                "confidence": float(conf),
                "polygon": [[float(p[0]), float(p[1])] for p in poly]
            })
    return lines

@app.route("/ocr", methods=["POST"])
def ocr_endpoint():
    try:
        if "image" not in request.files:
            return jsonify({"error": "missing file field 'image'"}), 400
        f = request.files["image"]
        raw = f.read()
        pil = Image.open(io.BytesIO(raw)).convert("RGB")
        width, height = pil.size

        lines = ocr_image(pil)

        # 相对坐标，前端不受缩放影响
        for l in lines:
            l["polygon_norm"] = [[x/width, y/height] for (x,y) in l["polygon"]]

        blocks = lines_to_blocks(lines)
        for b in blocks:
            x1,y1,x2,y2 = b["display_box"]
            b["display_box_norm"] = [x1/width, y1/height, x2/width, y2/height]

        return jsonify({
            "width": width, "height": height,
            "lines": lines,
            "blocks": blocks
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # 开发环境直接跑；部署建议用 gunicorn/uwsgi
    app.run(host="0.0.0.0", port=8001)




import requests
from rapidfuzz import fuzz, process as rf_process
import unicodedata

class DocumentProcessor:
    def __init__(self, model="miniCPM"):
        self.model = model
        self.vlm_url = settings.VLM_API_URL
        self.ocr_url = settings.OCR_API_URL  # 新增

    # ... 你原有的函数省略 ...

    def process_document(self, image_path: str) -> dict:
        try:
            start = time.time()
            doc_type = self.classify_document(image_path)
            extraction_results = self.extract_with_vlm(image_path, doc_type)
            final_results = self.clean_json_output(extraction_results['results'])

            # ① 调用 OCR 服务，拿到坐标
            ocr_payload = self.call_ocr_api(image_path)
            # ② 将实体值匹配回 OCR 块/行，得到坐标用于前端展示
            entity_positions = self.map_entities_to_positions(final_results, ocr_payload)

            return {
                "success": True,
                "processing_time": round(time.time() - start, 2),
                "doc_type": doc_type,
                "results": final_results,
                "ocr": {
                    "image_size": {"width": ocr_payload["width"], "height": ocr_payload["height"]},
                    "entity_positions": entity_positions  # 前端可直接用来画框
                }
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def call_ocr_api(self, image_path: str) -> dict:
        files = {'image': open(image_path, 'rb')}
        r = requests.post(self.ocr_url, files=files, timeout=30)
        r.raise_for_status()
        return r.json()

    # —— 文本规范化 & 匹配 —— #
    @staticmethod
    def _normalize(s: str) -> str:
        if not s:
            return ""
        # NFKC 统一全半角；去空白；小写
        s = unicodedata.normalize("NFKC", s)
        s = s.strip().lower()
        # 去除所有空白字符进行“宽松匹配”
        s = "".join(ch for ch in s if not ch.isspace())
        return s

    def map_entities_to_positions(self, entities: dict, ocr_payload: dict) -> dict:
        """
        entities: { entity_name: value, ... }
        返回：{ entity_name: { lines:[{polygon_norm,...}], display_box_norm:[x1,y1,x2,y2], confidence: float, reading_order:[int,...] } }
        """
        blocks = ocr_payload.get("blocks", [])
        results = {}

        # 预备：对每个块，建立 “块级规范化字符串” 以及 “行级索引映射”
        # 行级索引映射：把块text拼接时记录每行在拼接文本中的起止，用于命中后还原到哪些行。
        for entity_name, entity_value in entities.items():
            target = self._normalize(str(entity_value))
            if not target:
                results[entity_name] = None
                continue

            best = None
            for b in blocks:
                block_text = b.get("text", "")
                lines = b.get("lines", [])
                # 拼接文本时记录行跨度
                concat = ""
                spans = []
                for ln in lines:
                    start = len(concat)
                    t = ln["text"]
                    concat += t + " "
                    end = len(concat)
                    spans.append((start, end, ln))  # (start, end, 行对象)

                concat_norm = self._normalize(concat)

                # 1) 精确子串匹配（在去空白的 concat_norm 中）
                idx = concat_norm.find(target)
                score = 0
                hit_lines = []
                if idx != -1:
                    # 将“规范化索引”粗略映射回原串区间：
                    # 用比例近似（简单且够用）；需要更精确可保留去空白前后的字符映射表。
                    ratio = len(concat) / max(1, len(concat_norm))
                    est_start = int(idx * ratio)
                    est_end   = int((idx + len(target)) * ratio)

                    # 命中的行集合
                    for (s, e, ln) in spans:
                        if not (est_end <= s or est_start >= e):  # 有重叠即算命中
                            hit_lines.append(ln)
                    score = 100  # 精确命中
                else:
                    # 2) 宽松相似度（适配轻微差异）
                    score = fuzz.partial_ratio(target, concat_norm)
                    # 用“最长相似子串”的粗定位：简单做法—相似度高则默认整块行作为候选
                    # 你也可以用 rapidfuzz.process.extractOne 拿子串起止，这里先保持轻量策略
                    if score >= 85:
                        hit_lines = lines[:]

                if hit_lines and (best is None or score > best["score"]):
                    # 聚合 display_box
                    all_pts = []
                    confs = []
                    ro = []
                    for idx_ln, ln in enumerate(lines):
                        if ln in hit_lines:
                            ro.append(idx_ln)
                            confs.append(ln.get("confidence", 0.0))
                            for p in ln.get("polygon_norm") or []:
                                # polygon_norm 已是相对坐标
                                all_pts.append(p)
                    if all_pts:
                        xs = [p[0] for p in all_pts]
                        ys = [p[1] for p in all_pts]
                        display_box_norm = [min(xs), min(ys), max(xs), max(ys)]
                    else:
                        display_box_norm = b.get("display_box_norm")

                    avg_conf = float(np.mean(confs)) if confs else 0.0
                    best = {
                        "block_id": b["block_id"],
                        "lines": [{
                            "text": ln["text"],
                            "confidence": ln.get("confidence", 0.0),
                            "polygon_norm": ln.get("polygon_norm"),
                            "polygon": ln.get("polygon")
                        } for ln in hit_lines],
                        "display_box_norm": display_box_norm,
                        "confidence": avg_conf,
                        "reading_order": sorted(ro),
                        "score": score
                    }

            results[entity_name] = best

        return results
