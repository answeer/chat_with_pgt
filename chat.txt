Language Modeling Loss (
L
m
l
m
L 
mlm
​	
 ):

L
m
l
m
=
−
∑
i
log
⁡
P
student
(
w
i
∣
w
context
)
L 
mlm
​	
 =−∑ 
i
​	
 logP 
student
​	
 (w 
i
​	
 ∣w 
context
​	
 )

where 
w
i
w 
i
​	
  is the masked word, 
w
context
w 
context
​	
  is its context, and 
P
student
P 
student
​	
  is the prediction probability by the student model.

Distillation Loss (
L
c
e
L 
ce
​	
 ):

L
c
e
=
−
∑
i
t
i
log
⁡
(
s
i
)
L 
ce
​	
 =−∑ 
i
​	
 t 
i
​	
 log(s 
i
​	
 )

where 
t
i
t 
i
​	
  is the soft target probability from the teacher model, and 
s
i
s 
i
​	
  is the prediction probability by the student model.

To smooth the output distribution, the softened softmax is used:

p
i
=
exp
⁡
(
z
i
/
T
)
∑
j
exp
⁡
(
z
j
/
T
)
p 
i
​	
 = 
∑ 
j
​	
 exp(z 
j
​	
 /T)
exp(z 
i
​	
 /T)
​	
 

where 
T
T is the temperature parameter and 
z
i
z 
i
​	
  are the logits from the model.

Cosine-Distance Loss (
L
c
o
s
L 
cos
​	
 ):

L
c
o
s
=
1
−
cos
⁡
(
h
teacher
,
h
student
)
L 
cos
​	
 =1−cos(h 
teacher
​	
 ,h 
student
​	
 )

where 
h
teacher
h 
teacher
​	
  and 
h
student
h 
student
​	
  are the hidden state vectors from the teacher and student models, respectively, and 
cos
⁡
cos represents the cosine similarity between the two vectors.

The final training objective is a linear combination of these three loss functions:

L
=
α
L
m
l
m
+
β
L
c
e
+
γ
L
c
o
s
L=αL 
mlm
​	
 +βL 
ce
​	
 +γL 
cos
​	
 

where 
α
α, 
β
β, and 
γ
γ are weight coefficients used to balance the contributions of the three loss functions in the total loss.
