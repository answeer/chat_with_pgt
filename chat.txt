import os
import traceback
import mimetypes
import time
import json
import psutil
import numpy as np
import matplotlib.pyplot as plt
from threading import Thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from llm_sanitation.utils.callback import Callback
from llm_sanitation.utils.checks import Checks
from llm_sanitation.utils.error_codes import ERROR_CODE_DICT
from llm_sanitation.logging.logging_setup import LogUtil, LogType, LogLevel

class Action:
    def __init__(self, default_scanner_path, async_mode=False, **kwargs):
        """
        :param default_scanner_path: Path to the default scanners configuration
        :param async_mode: Boolean flag to enable asynchronous execution (default is False)
        :param kwargs: Other keyword arguments
        """
        LogUtil.log(LogType.TRANSACTION, LogLevel.INFO, "Running Initialized")
        self.response = Callback(**kwargs)
        self.check = Checks(self.response)
        self.kwargs = kwargs
        self.combined_result = []
        self.nstp_result = []
        self.default_scanners = self.load_default_scanners(default_scanner_path)  # Load JSON config
        self.async_mode = async_mode  # Store the async flag

    @staticmethod
    def load_default_scanners(config_path):
        """Loads default scanner configurations from a JSON file."""
        try:
            with open(config_path, "r") as file:
                return json.load(file)
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, f"Error loading default scanners: {e}")
            return {}

    def determine_task_group(self,data):
        """Determines the task group based on the input data type."""
        if isinstance(data, str):
            # Check if the data is a filepath
            if os.path.isfile(data):
                # Determine the MIME type
                mime_type, _ = mimetypes.guess_type(data)
                if mime_type and mime_type.startswith('image'):
                    return 'image_bounding'  # Image files
                else:
                    return 'file_bounding'  # All other file types
            else:
                # Plain string, not a filepath
                return 'text_bounding'
        elif isinstance(data, (int, float)):
            # If data is numeric
            return 'numeric_bounding'
        else:
            raise ValueError("Unsupported data type for task group determination.")
        
    def merge_execution_plan(self, task_group, execution_plan):
        """Merge default scanners with additional execution plan."""
        default_plan = self.default_scanners.get(task_group, {})
        return {**default_plan, **execution_plan}  # Default scanners take precedence


    def run_scanner(self, data, scanner_nm, param, **kwargs):
        """Dynamically imports and runs the specified scanner with provided parameters."""
        try:
            scanners = __import__("llm_sanitation.scanners", fromlist=[scanner_nm])
            scanner = getattr(scanners, scanner_nm)
            scanner_obj = scanner(**param)
            result = scanner_obj.validate(data, **self.kwargs)
            self.combined_result.append(result)
            self.nstp_result.append(result.get("NSTP"))
            return result
        except (ImportError, AttributeError) as e:
            error_code = "FRDIOS9000"
            error_message = ERROR_CODE_DICT[error_code].format(str(e))
            self.response.form_response(error_code, error_message)
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=self.kwargs['job_id'], error=error_message)
            return None

    def get_result(self):
        """Returns all combined results from the scanners."""
        return self.combined_result

    def get_nstp_result(self):
        """Returns NSTP results from each scanner."""
        return self.nstp_result

    def run_scanner_task(self,data,scanner_nm,param,kwargs):
        return self.run_scanner(data,scanner_nm,param,**kwargs)

    def run_action(self, execution_plan, data, save_path):
        """Executes the scanning process based on the provided execution plan and scanner type."""
        try:
            start_time = time.time()  # Record the start time for performance testing
            
            error_code = ""
            error_message = ""
            job_id = self.kwargs['job_id']
            task_group = self.determine_task_group(data)
            self.kwargs['task_group'] = task_group
            execution_plan = self.merge_execution_plan(task_group, execution_plan)

            # Run pre-check based on task group
            if task_group == "file_bounding":
                self.check.file_exists(data)
            elif task_group == "image_bounding":
                self.check.check_image(data)
            elif task_group == "text_bounding":
                self.check.check_text(data)
            elif task_group == "numeric_bounding":
                self.check.check_payload(data)
            else:
                error_code = "FRDIOS0006"
                error_message = ERROR_CODE_DICT[error_code].format(str(task_group))
                LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                return None, error_code, error_message
            
            if self.response.status == "failed":
                error_code = self.response.error_code
                error_message = self.response.error_message
                LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                return None, error_code, error_message
            
            if self.async_mode:  # 如果启用了异步模式，使用 multiprocessing
                with ThreadPoolExecutor(max_workers=8) as executor:
                    futures = []
                    for _ in range(20):
                        for scanner_nm, param in execution_plan.items():
                            futures.append(executor.submit(self.run_scanner, data, scanner_nm, param, **self.kwargs))
                    # Wait for all threads to complete and collect results
                    for future in as_completed(futures):
                        result = future.result()
                    if result is None and self.response.status == "failed":
                        error_code = self.response.error_code
                        error_message = self.response.error_message
                        LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                        return None, error_code, error_message
            else:  # If async mode is not enabled, run sequentially
                for _ in range(20): 
                    for scanner_nm, param in execution_plan.items():
                    # for _ in range(8):
                        result = self.run_scanner(data, scanner_nm, param, **kwargs)
                if self.response.status == "failed":
                    error_code = self.response.error_code
                    error_message = self.response.error_message
                    LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                    return None, error_code, error_message
                
            end_time = time.time()  # Record the end time for performance testing
            execution_time = end_time - start_time
            LogUtil.log(LogType.TRANSACTION, LogLevel.INFO, f"Execution completed in {execution_time:.2f} seconds.")
                
            return self.combined_result,error_code,error_message

        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=str(e))
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, traceback.format_exc())
            error_code = "FRDIOS9000"
            error_message = ERROR_CODE_DICT[error_code].format(str(e))
            return None, error_code, error_message
        
        finally:
            if save_path:
                response_json_path = os.path.join(save_path, task_group + "_response.json")
                self.response.save_results(combined_result=self.combined_result)
                self.response.return_response(response_json_path, error_code, error_message)

def monitor_cpu_usage(interval=1, duration=10):
    cpu_usage = []
    start_time = time.time()
    while time.time() - start_time < duration:
        cpu_usage.append(psutil.cpu_percent(interval=interval))
    return cpu_usage


def plot_cpu_utilization(cpu_usage, mode):
    # 平滑数据，使用卷积实现移动平均
    smoothed_cpu_usage = np.convolve(cpu_usage, np.ones(5)/5, mode='valid')

    plt.figure(figsize=(10, 6))
    plt.plot(smoothed_cpu_usage, label=f"CPU Utilization ({mode})", color='blue', linewidth=2)

    # 填充区域
    plt.fill_between(range(len(smoothed_cpu_usage)), smoothed_cpu_usage, color='blue', alpha=0.2)

    # 标注数据点
    for i in range(0, len(smoothed_cpu_usage), 10):  # 每隔10个点标注一次
        plt.annotate(f'{smoothed_cpu_usage[i]:.1f}%', (i, smoothed_cpu_usage[i]),
                     textcoords="offset points", xytext=(0,5), ha='center', fontsize=8, color='black')

    # 设置图表样式
    plt.title(f"CPU Utilization - {mode} Execution", fontsize=14)
    plt.xlabel("Time (in 0.5s intervals)", fontsize=12)
    plt.ylabel("CPU Usage (%)", fontsize=12)
    plt.legend()
    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    
    # 显示图表
    plt.tight_layout()
    # plt.show()
    plt.savefig('cpu_usage_for_{}.png'.format(mode))


def execute_with_monitoring(action_obj, execution_plan, data, save_path, mode):
    monitor_thread = Thread(target=monitor_cpu_usage, args=(0.1,10))
    monitor_thread.start()

    start_time = time.time()
    action_obj.run_action(execution_plan, data, save_path)
    end_time = time.time()

    monitor_thread.join()
    print(f"Execution time for {mode}: {end_time - start_time:.2f} seconds")
    plot_cpu_utilization(monitor_cpu_usage(0.1,10), mode)

if __name__ == "__main__":
    payload = {
            "job_params": {
                "jobid": "JID-54ca58ba-c495-11ed-b20c-0a586e830578",
                "task_id": "TID-54ca58ba-c495-11ed-b20c-0a586e830578",
                "app_name": "synthesizer",
                "use_case": "aadhar_redact",
                "save_path": r"C:\Users\1657820\Desktop\51433-swoosh-io-bounding",
            },
            "service_params": {
                "callback_url": "http://service-swoosh-orchestrator.swoosh-dev.svc.cluster.local:8889/api/v1/callback",
                "job_object": {
                    "io": "i",
                    "policy_id": "policy_00001",
                    "data": """
        The tumultuous tempest of existential angst, swirling like a relentless vortex within the depths of my weary soul, engulfs me in an
        overwhelming deluge of despair and desolation, as if the very fabric of reality conspires to suffocate my hopes and dreams beneath
        its crushing weight.
        """,
                    "execution_plan": {}
                }
            }
        }

    kwargs = {
        "job_id": payload["job_params"].get("jobid", "NA"),
        "task_id": payload["job_params"].get("task_id", "NA"),
        "use_case": payload["job_params"].get("use_case", "NA"),
        "io": payload["service_params"]["job_object"].get("io", "NA"),
        "policy_id": payload["service_params"]["job_object"].get("policy_id", "NA"),
    }
    default_scanner_path = "llm_sanitation\configs\default_scanners.json"
    print("Sequential Execution:")
    action_seq = Action(default_scanner_path, async_mode=False, **kwargs)
    execute_with_monitoring(action_seq, payload["service_params"]["job_object"]["execution_plan"],
                            payload["service_params"]["job_object"]["data"],
                            payload["job_params"]["save_path"], "Sequential")
    
    print("Parallel Execution:")
    action_par = Action(default_scanner_path, async_mode=True, **kwargs)
    execute_with_monitoring(action_par, payload["service_params"]["job_object"]["execution_plan"],
                            payload["service_params"]["job_object"]["data"],
                            payload["job_params"]["save_path"], "Parallel")
