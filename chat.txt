import os
import traceback
import mimetypes
import time
import json
import psutil
import matplotlib.pyplot as plt
from threading import Thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from llm_sanitation.utils.callback import Callback
from llm_sanitation.utils.checks import Checks
from llm_sanitation.utils.error_codes import ERROR_CODE_DICT
from llm_sanitation.logging.logging_setup import LogUtil, LogType, LogLevel

class Action:
    def __init__(self, default_scanner_path, async_mode=False, **kwargs):
        """
        :param default_scanner_path: Path to the default scanners configuration
        :param async_mode: Boolean flag to enable asynchronous execution (default is False)
        :param kwargs: Other keyword arguments
        """
        LogUtil.log(LogType.TRANSACTION, LogLevel.INFO, "Running Initialized")
        self.response = Callback(**kwargs)
        self.check = Checks(self.response)
        self.kwargs = kwargs
        self.combined_result = []
        self.nstp_result = []
        self.default_scanners = self.load_default_scanners(default_scanner_path)  # Load JSON config
        self.async_mode = async_mode  # Store the async flag
    @staticmethod
    def load_default_scanners(config_path):
        """Loads default scanner configurations from a JSON file."""
        try:
            with open(config_path, "r") as file:
                return json.load(file)
        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, f"Error loading default scanners: {e}")
            return {}

    def determine_task_group(self,data):
        """Determines the task group based on the input data type."""
        if isinstance(data, str):
            # Check if the data is a filepath
            if os.path.isfile(data):
                # Determine the MIME type
                mime_type, _ = mimetypes.guess_type(data)
                if mime_type and mime_type.startswith('image'):
                    return 'image_bounding'  # Image files
                else:
                    return 'file_bounding'  # All other file types
            else:
                # Plain string, not a filepath
                return 'text_bounding'
        elif isinstance(data, (int, float)):
            # If data is numeric
            return 'numeric_bounding'
        else:
            raise ValueError("Unsupported data type for task group determination.")
        
    def merge_execution_plan(self, task_group, execution_plan):
        """Merge default scanners with additional execution plan."""
        default_plan = self.default_scanners.get(task_group, {})
        return {**default_plan, **execution_plan}  # Default scanners take precedence


    def run_scanner(self, data, scanner_nm, param, **kwargs):
        """Dynamically imports and runs the specified scanner with provided parameters."""
        try:
            scanners = __import__("llm_sanitation.scanners", fromlist=[scanner_nm])
            scanner = getattr(scanners, scanner_nm)
            scanner_obj = scanner(**param)
            result = scanner_obj.validate(data, **self.kwargs)
            self.combined_result.append(result)
            self.nstp_result.append(result.get("NSTP"))
            return result
        except (ImportError, AttributeError) as e:
            error_code = "FRDIOS9000"
            error_message = ERROR_CODE_DICT[error_code].format(str(e))
            self.response.form_response(error_code, error_message)
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=self.kwargs['job_id'], error=error_message)
            return None

    def get_result(self):
        """Returns all combined results from the scanners."""
        return self.combined_result

    def get_nstp_result(self):
        """Returns NSTP results from each scanner."""
        return self.nstp_result

    def run_action(self, execution_plan, data, save_path):
        """Executes the scanning process based on the provided execution plan and scanner type."""
        cpu_usage_data = []
        start_time = time.time()
        try:            
            error_code = ""
            error_message = ""
            job_id = self.kwargs['job_id']
            task_group = self.determine_task_group(data)
            self.kwargs['task_group'] = task_group
            execution_plan = self.merge_execution_plan(task_group, execution_plan)

            # Run pre-check based on task group
            if task_group == "file_bounding":
                self.check.file_exists(data)
            elif task_group == "image_bounding":
                self.check.check_image(data)
            elif task_group == "text_bounding":
                self.check.check_text(data)
            elif task_group == "numeric_bounding":
                self.check.check_payload(data)
            else:
                error_code = "FRDIOS0006"
                error_message = ERROR_CODE_DICT[error_code].format(str(task_group))
                LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                return None, error_code, error_message
            
            if self.response.status == "failed":
                error_code = self.response.error_code
                error_message = self.response.error_message
                LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                return None, error_code, error_message
            
            if self.async_mode:  # If async mode is enabled, use ThreadPoolExecutor
                with ThreadPoolExecutor(max_workers=100) as executor:
                     for _ in range(100):
                        futures = []
                        for scanner_nm, param in execution_plan.items():
                            futures.append(executor.submit(self.run_scanner, data, scanner_nm, param, **self.kwargs))

                        # Wait for all threads to complete and collect results
                        for future in as_completed(futures):
                            result = future.result()
                            cpu_usage_data.append((time.time() - start_time, psutil.cpu_percent(interval=0.1)))
                    if result is None and self.response.status == "failed":
                        error_code = self.response.error_code
                        error_message = self.response.error_message
                        LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                        return None, error_code, error_message
            else:  # If async mode is not enabled, run sequentially
                 for _ in range(100):
                    for scanner_nm, param in execution_plan.items():
                        result = self.run_scanner(data, scanner_nm, param, **kwargs)
                        cpu_usage_data.append((time.time() - start_time, psutil.cpu_percent(interval=0.1)))
                
                if self.response.status == "failed":
                    error_code = self.response.error_code
                    error_message = self.response.error_message
                    LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=error_message)
                    return None, error_code, error_message
                
            end_time = time.time()  # Record the end time for performance testing
            execution_time = end_time - start_time
            LogUtil.log(LogType.TRANSACTION, LogLevel.INFO, f"Execution completed in {execution_time:.2f} seconds.")
                
            return self.combined_result,error_code,error_message, cpu_usage_data

        except Exception as e:
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, "Running ended with error", jid=job_id, error=str(e))
            LogUtil.log(LogType.ERROR, LogLevel.ERROR, traceback.format_exc())
            error_code = "FRDIOS9000"
            error_message = ERROR_CODE_DICT[error_code].format(str(e))
            return None, error_code, error_message, cpu_usage_data
        
        finally:
            if save_path:
                response_json_path = os.path.join(save_path, task_group + "_response.json")
                self.response.save_results(combined_result=self.combined_result)
                self.response.return_response(response_json_path, error_code, error_message)

def plot_cpu_usage(sequential_data, parallel_data):
    """Plot CPU usage data for sequential and parallel executions."""
    plt.figure(figsize=(12, 6))

    # Plot sequential data
    if sequential_data:
        times, cpu_usages = zip(*sequential_data)
        plt.plot(times, cpu_usages, label='Sequential', marker='o', linestyle='-')

    # Plot parallel data
    if parallel_data:
        times, cpu_usages = zip(*parallel_data)
        plt.plot(times, cpu_usages, label='Parallel (Async)', marker='x', linestyle='--')

    plt.xlabel("Time (seconds)")
    plt.ylabel("CPU Usage (%)")
    plt.title("CPU Usage Comparison: Sequential vs Parallel")
    plt.legend()
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    payload = {
            "job_params": {
                "jobid": "JID-54ca58ba-c495-11ed-b20c-0a586e830578",
                "task_id": "TID-54ca58ba-c495-11ed-b20c-0a586e830578",
                "app_name": "synthesizer",
                "use_case": "aadhar_redact",
                "save_path": r"C:\Users\1657820\Desktop\51433-swoosh-io-bounding",
            },
            "service_params": {
                "callback_url": "http://service-swoosh-orchestrator.swoosh-dev.svc.cluster.local:8889/api/v1/callback",
                "job_object": {
                    "io": "i",
                    "policy_id": "policy_00001",
                    "data": "what are you doing",
                    "execution_plan": {}
                }
            }
        }

    kwargs = {
        "job_id": payload["job_params"].get("jobid", "NA"),
        "task_id": payload["job_params"].get("task_id", "NA"),
        "use_case": payload["job_params"].get("use_case", "NA"),
        "io": payload["service_params"]["job_object"].get("io", "NA"),
        "policy_id": payload["service_params"]["job_object"].get("policy_id", "NA"),
    }
    default_scanner_path = "llm_sanitation\configs\default_scanners.json"

    print("Sequential Execution:")
    action_seq = Action(default_scanner_path, async_mode=False, **kwargs)
    result_seq, error_code_seq, error_message_seq, cpu_data_seq = action_seq.run_action(
            payload["service_params"]["job_object"]["execution_plan"],
            payload["service_params"]["job_object"]["data"],
            payload["job_params"]["save_path"]
        )
    print("Sequential Action completed")

    print("Parallel Execution:")
    action_par = Action(default_scanner_path, async_mode=True, **kwargs)
    result_par, error_code_par, error_message_par, cpu_data_par  = action_par.run_action(
            payload["service_params"]["job_object"]["execution_plan"],
            payload["service_params"]["job_object"]["data"],
            payload["job_params"]["save_path"]
        )
    print("Parallel Action completed")


    plot_cpu_usage(cpu_data_seq, cpu_data_par)
