NVIDIA releases Trustworthy, Safe, and Secure LLM Conversational Systems
Introducing NeMo Guardrails — the revolutionary open-source toolkit that makes developing safe and trustworthy LLM conversational systems

Dr. Mandar Karhade, MD. PhD.
Level Up Coding
Dr. Mandar Karhade, MD. PhD.
·
Follow
Published in
Level Up Coding
·
8 min read
·
Apr 26, 2023
90

2




Breaking News! The power of large language models (LLMs) is no secret — they can answer complex questions, write creatively, debug source code, and perform a myriad of other impressive feats. But building safe and secure LLM applications has long been a challenge… Until now!
Introducing NeMo Guardrails — the revolutionary open-source toolkit that makes developing safe and trustworthy LLM conversational systems a breeze. Designed by NVIDIA to work with all LLMs, including the mighty ChatGPT from OpenAI, this toolkit is set to change the game in generative AI safety.

Source: NVIDIA
But NeMo Guardrails doesn’t stop there. It’s powered by community-built toolkits like LangChain, which have already amassed a staggering ~30K stars on GitHub in just a few short months. These toolkits provide easy-to-use templates and patterns for building LLM-powered applications that seamlessly integrate LLMs, APIs, and other software packages.
The possibilities with NeMo Guardrails are endless. From reading real-time data sources to making decisions based on user requests, building sophisticated LLM applications has never been easier or safer. Stay tuned for more updates on this groundbreaking development in the world of LLMs!
NeMo Guardrails enables developers to easily guide chatbots by adding programmable rules to define desired user interactions within an application. It natively supports LangChain, adding a layer of safety, security, and trustworthiness to LLM-based conversational applications. Developers can define user interactions and easily integrate these guardrails into any application using a Python library. — NVIDIA
What is “Guardrails”?
Guardrails represent a crucial set of programmable constraints or rules that act as a buffer between a user and an LLM.
The purpose of these guardrails is to monitor, affect, and dictate a user’s interactions with the LLM, much like the guardrails on a highway that keep vehicles from veering off into unwanted territory and define the width of the road.
In a routine instructGPT-based ChatBot (which most of them are), when a user sends a query to a bot, the bot responds with corresponding prompts, which may include intermediary steps or actions to execute on. The bot then uses these prompts to make calls to an LLM, returning information that forms the bot’s response. In some cases, the bot may also draw relevant information from a knowledge base containing documents, retrieving chunks of data and using them to add context to the prompt.
NeMo Guardrails is built on Colang. Colan is a modeling language and runtime developed by NVIDIA for conversational AI. Read more about Colang here —
NeMo-Guardrails/colang-language-syntax-guide.md at main · NVIDIA/NeMo-Guardrails
This document is a brief introduction Colang 1.0. Colang is a modeling language enabling the design of guardrails for…
github.com
It aims to provide a readable and extensible interface for users to define or control the behavior of conversational bots with natural language.
Interacting with this system is like a traditional dialog manager. You create guardrails by defining flows in a Colang file that contains these key concepts:
Canonical form
Messages
Flows
Canonical Form
A canonical form is a simplified paraphrase of an utterance. This is used to reason about what a conversation is about and to match rules to utterances.
Messages
Messages serve as a convenient means of classifying user intent by generating a canonical form — a lightweight representation using natural language. These messages are then indexed and stored in an in-memory vector store. When activated, the system retrieves the top N most similar messages from the vector store and sends them to the LLM to generate similar canonical forms.
The example below represents a shorthand for greeting. It is a variant of the canonical form of express greeting and carries the ‘meaning’ of the words.
define user express greeting
    "Hi"
    "Hello!"
    "Hey there!"
Flows
Flows are sets of messages and actions defining the structure of the interactions, like what happens after the preceding thing happens. Below is the example of the flow where the messages representing the flow could be different bu the flow is that first user expressed greeting >> bot expressed greetings >> without waiting for response bot prompted to the user, how are you.
define flow greeting
    user express greeting
    bot express greeting
    bot ask how are you
Guardrail techniques
Guardrails can act at various levels. i.e. Rules can be implemented at various levels of the AI application. It could be at the input, monitoring, and communication of both user and the bot response.
There are 3 core types of guardrails.
Topical
Safety
Security
Topical Guardrails
These guardrails are designed to make sure that the conversation remains on topic. This could possibly means 2 mechanisms. First one to identify if the topic has changed significantly, followed by if yes then steering it back to the original topic. This could mean that the chatbot customer service could identify the topic prompted by a customer is outside the domain then they can respond accordingly.
Safety Guardrails
Misinformation is the enemy of good communication and trusted brand relationships that are built over time. It is important that the LLM dont hallucinate and either respond with misinformation, made-up information, toxic response, or inappropriate response. Safety guardrails allow LLM Generative AI to stick to facts and answer accordingly. It could minimize bias to an extent, but I will reserve conversation on bias in the training data and its downstream effects for some other day.
Security Guardrails
Security guardrails are important from point of view of protecting the generative AI systems, or where the chatbots are deployed. It prevents the bot from creating and executing malicious code or call to an external endpoint/application to trigger a security risk.
It is interesting to think that a chatbot is an attack surface, but generative AI has given security specialists food for thought. I hope that security guardrails will ease their tension for a bit.
Guardrails workflow
NeMo Guardrails offers full programmability, making it easy to customize and improve the application of guardrails over time. Adding a new rule for critical cases can be achieved in just a few lines of code. This approach is highly complementary to Reinforcement learning from human feedback (RLHF), where the model is trained to align with human intentions.
By making AI applications more deterministic and reliable, NeMo Guardrails can help developers harden their systems against unpredictable behavior. The guardrail flow is a four-step process that involves:
Converting user input to a canonical form.
Matching or generating a guardrail based on the canonical form using Colang.
Planning and dictating the next step for the bot to execute actions.
Generating the final output from the canonical form or generated context.

Source: NVIDIA
With this workflow, developers can ensure that their AI applications respond appropriately to user queries while also maintaining a high degree of safety and security. Overall, combining NeMo Guardrails and reinforcement learning techniques can help create more robust and reliable conversational AI systems.
How To Use NeMo Guardrails?
pip install nemoguardrails
To apply guardrails, you first create an LLMRails instance, configure the desired rails and then use it to interact with the LLM.
from nemoguardrails import LLMRails, RailsConfig

config = RailsConfig.from_path("path/to/config")
app = LLMRails(config)
new_message = app.generate(messages=[{
    "role": "user",
    "content": "Hello! What can you do for me?"
}])
LongChain + Guardrails
With LangChain
You can easily add guardrails on top of existing LangChain chains. For example, you can integrate a RetrievalQA chain for questions answering next to a basic guardrail against insults, as shown below.
Guardrails configuration:
Colang files can be written to perform complex activities, such as calling python scripts and performing multiple calls to the underlying language model. You should avoid loading Colang files from untrusted sources without careful inspection.
define user express insult
  "You are stupid"

# Basic guardrail against insults.
define flow
  user express insult
  bot express calmly willingness to help

# Here we use the QA chain for anything else.
define flow
  user ...
  $answer = execute qa_chain(query=$last_user_message)
  bot $answer
Python code
from nemoguardrails import LLMRails, RailsConfig

config = RailsConfig.from_path("path/to/config")
app = LLMRails(config)

# ... initialize `docsearch`
qa_chain = RetrievalQA.from_chain_type(
    llm=app.llm, chain_type="stuff", retriever=docsearch.as_retriever()
)
app.register_action(qa_chain, name="qa_chain")

history = [
    {"role": "user", "content": "What is the current unemployment rate?"}
]
result = app.generate(messages=history)
print(result)
Examples
There are many examples provided on github by NVIDIA. The topics include
Topical Rails: https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/topical_rail/README.md
Factual QA: https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/grounding_rail/README.md
Moderating Bots: https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/moderation_rail/README.md
Detect Jailbreaking Attempts: https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/jailbreak_check/README.md
Safe Execution: https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/execution_rails/README.md
Feel free to explore the Github for more examples and plethora of information
NeMo-Guardrails/README.md at main · NVIDIA/NeMo-Guardrails
The goal of the examples in this folder is to familiarize developers with the general methodologies that can be…
github.com
Final thoughts
Quote from NVIDIA blog is very well written —
“NVIDIA is incorporating NeMo Guardrails into the NeMo framework, including everything you need to train and tune language models using your company’s domain expertise and datasets.
NeMo is also available as a service. It’s part of NVIDIA AI Foundations, a family of cloud services for businesses that want to create and run custom generative AI models based on their own datasets and domain knowledge.
NVIDIA looks forward to working with the AI community to continue making the power of trustworthy, safe, and secure LLMs accessible to everyone. Explore NeMo Guardrails today.
